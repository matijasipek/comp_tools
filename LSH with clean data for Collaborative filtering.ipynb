{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a2a8849",
   "metadata": {},
   "source": [
    "Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cccdca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "import time\n",
    "from datasketch import MinHash, MinHashLSHForest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e08282d",
   "metadata": {},
   "source": [
    "Read in users play counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e1b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 1\n",
    "# start_time = time.time()\n",
    "# m = pd.read_csv('usertriplets_clean.csv')\n",
    "# m = m.pivot(index='userID', columns='songID', values='play_count')\n",
    "# print('It took %s seconds.' %(time.time()-start_time))\n",
    "# #m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a9d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 2\n",
    "# start_time = time.time()\n",
    "# frame = pd.read_csv('usertriplets_clean.csv')\n",
    "\n",
    "# if not 'user_profiles' in globals():\n",
    "#     person_u = list(frame.userID.unique())\n",
    "#     thing_u = list(frame.songID.unique())\n",
    "\n",
    "#     data = frame['play_count'].tolist()\n",
    "#     row = frame.userID.astype('category').cat.codes\n",
    "#     col = frame.songID.astype('category').cat.codes\n",
    "#     sparse_matrix = csr_matrix((data, (row, col)), shape=(len(person_u), len(thing_u)))\n",
    "#     user_profiles = pd.DataFrame.sparse.from_spmatrix(sparse_matrix, index=person_u, columns=thing_u)\n",
    "# print('It took %s seconds.' %(time.time()-start_time))\n",
    "# #user_profiles.shape()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8eb379",
   "metadata": {},
   "source": [
    "The second method improves runtime 10X times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05182655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #u = u.sparse.to_dense()\n",
    "# start_time = time.time()\n",
    "\n",
    "# similarities,indices = get_similarusers( '5a905f000fc1ff3df7ca807d57edb608863db05d', u  , similarity_metric = 'correlation', k = 4)\n",
    "# print('It took %s seconds.' %(time.time()-start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b12c3d",
   "metadata": {},
   "source": [
    "Since classic collaborative filtering method is too memory heavy (and cant run on my pc); try LSH approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ffb50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def triplets_to_utility_matrix(path_to_file ):\n",
    "    start_time = time.time()\n",
    "    frame = pd.read_csv(path_to_file)\n",
    "    person_u = list(frame.userID.unique())\n",
    "    thing_u = list(frame.songID.unique())\n",
    "    data = frame['play_count'].tolist()\n",
    "    row = frame.userID.astype('category').cat.codes\n",
    "    col = frame.songID.astype('category').cat.codes\n",
    "    sparse_matrix = csr_matrix((data, (row, col)), shape=(len(person_u), len(thing_u)))\n",
    "    user_profiles = pd.DataFrame.sparse.from_spmatrix(sparse_matrix, index=person_u, columns=thing_u)\n",
    "    # Creater smaller dataset\n",
    "    split = int(386670/5)\n",
    "    u1 = user_profiles[:split]\n",
    "    u1 = u1.sparse.to_dense()\n",
    "    # Normalize play counts\n",
    "    u1.replace(0, np.nan, inplace=True)\n",
    "    u1 = u1.sub(u1.mean(axis=1, skipna=True), axis=0) # substract from each cell the row mean\n",
    "    # if value >=0 user likes song\n",
    "    u1[u1 >= 0] = 1\n",
    "    u1[u1 < 0] = 0\n",
    "    u1 = u1.T\n",
    "    # Keep users that have liked more than 4 songs \n",
    "    cols = np.where(u1.notna().sum() >= 4)\n",
    "    sub1 = u1.iloc[:, np.r_[cols]]\n",
    "    print('It took %s seconds.' %(time.time()-start_time))\n",
    "\n",
    "    return sub1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dcec235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 70.11948895454407 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Create utility matrix in chunks\n",
    "\n",
    "# m with user_profiles[:split]\n",
    "m = triplets_to_utility_matrix('data/usertriplets_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863e21f",
   "metadata": {},
   "source": [
    "Run the above function with different splitting strategy to create a bigger database in chunks as shown in the commented cell bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb093a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make changes to triplets_to_utility_matrix functions !\n",
    "#\n",
    "# # m2 with user_profiles[split:split*2]\n",
    "# m2 = triplets_to_utility_matrix('usertriplets_clean.csv')\n",
    "# # m3 with user_profiles[split*2:split*3]\n",
    "# m3 = triplets_to_utility_matrix('usertriplets_clean.csv')\n",
    "# # m4 with user_profiles[split*3:split*4]\n",
    "# m4 = triplets_to_utility_matrix('usertriplets_clean.csv')\n",
    "#\n",
    "# # Merging\n",
    "# m_total = pd.concat([m, m2, m3, m4], axis=1)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09d84c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97593c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database for the LSH algo\n",
    "def create_database(utility_matrix):\n",
    "    start_time = time.time()\n",
    "    cols = utility_matrix.columns.to_numpy() # the users \n",
    "    vectors_list = [cols[x].tolist() for x in utility_matrix.eq(1).to_numpy()] # each vector is a song, contains users that liked that song\n",
    "    df_new = pd.DataFrame(vectors_list, index = utility_matrix.index)\n",
    "    df_new_reduced = df_new.mask(df_new.eq('None')).dropna(how = 'all')\n",
    "    df_new_reduced['users'] = df_new_reduced[df_new_reduced.columns].apply( lambda x: ','.join(x.dropna().astype(str)),axis=1)\n",
    "    data = df_new_reduced['users'].copy()\n",
    "    database = pd.DataFrame(data)\n",
    "    print('It took %s seconds.' %(time.time()-start_time))\n",
    "\n",
    "    return database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0640fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 1.1063482761383057 seconds.\n"
     ]
    }
   ],
   "source": [
    "db = create_database(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c1dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#db_total = create_database(m_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967696c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab34f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items_from_user(userID, utility_matrix):\n",
    "    '''Given a user find the list of items he likes'''\n",
    "    #list containing the songs the user likes\n",
    "    songs = utility_matrix.index[utility_matrix[userID].eq(1)].tolist() \n",
    "    \n",
    "    return songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bc9b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess will split a string of text into individual tokens/shingles based on \",\".\n",
    "def preprocess(text):\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e5a40",
   "metadata": {},
   "source": [
    "Create MiniHash forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a38d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forest(data, perms):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    minhash = []\n",
    "    \n",
    "    for users in data['users']:\n",
    "        tokens = preprocess(users) # list of users\n",
    "        m = MinHash(num_perm=perms)\n",
    "        for s in tokens:\n",
    "            m.update(s.encode('utf8'))\n",
    "        minhash.append(m)\n",
    "        \n",
    "    forest = MinHashLSHForest(num_perm=perms)\n",
    "    \n",
    "    for i,m in enumerate(minhash):\n",
    "        forest.add(i,m)\n",
    "        \n",
    "    forest.index()\n",
    "    \n",
    "    print('It took %s seconds to build forest.' %(time.time()-start_time))\n",
    "    \n",
    "    return forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e302be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(song_profile, database, perms, num_results, forest): # song_profile in list form\n",
    "    \n",
    "    #song_profiles = get_items_from_user(userID, utility_matrix ,database)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    m = MinHash(num_perm=perms)\n",
    "    for users in song_profile:\n",
    "        m.update(users.encode('utf8'))\n",
    "        \n",
    "    idx_array = np.array(forest.query(m, num_results*2))\n",
    "    if len(idx_array) == 0:\n",
    "        return None # if your query is empty, return none\n",
    "    \n",
    "    #result = database.iloc[idx_array]['users']\n",
    "    result = database.iloc[idx_array]['users']\n",
    "\n",
    "    print('It took %s seconds to query forest.' %(time.time()-start_time))\n",
    "    \n",
    "    return result, idx_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611af04d",
   "metadata": {},
   "source": [
    "Choose parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84201a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Permutations\n",
    "permutations = 256\n",
    "\n",
    "#Number of recommendations to return\n",
    "num_recommendations = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce4efc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 5.261036396026611 seconds to build forest.\n"
     ]
    }
   ],
   "source": [
    "forest = get_forest(db, permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fea4047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(userID, utility_matrix, database, permutations, num_recommendations, forest):\n",
    "    items = get_items_from_user(userID, utility_matrix)\n",
    "    for i in range(0,len(items)):\n",
    "        # for each song user likes get the song's item_profile vector\n",
    "        item_profile = database.loc[items[i]] #.to_numpy()\n",
    "        # get recommendetions based on each song user likes\n",
    "        result, i = predict(item_profile, database, permutations, num_recommendations, forest)\n",
    "        print('\\n Top Recommendation(s) is(are) \\n', result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# userID = 'bf30441e24ef5326354295723d9fe1edf59b8554'\n",
    "# utility_matrix = m_total\n",
    "# database = db_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e80842e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.017992734909057617 seconds to query forest.\n",
      "\n",
      " Top Recommendation(s) is(are) \n",
      " SOVGXVD12A58A7D23B    bf30441e24ef5326354295723d9fe1edf59b8554,722c6...\n",
      "Name: users, dtype: object\n",
      "It took 0.009572744369506836 seconds to query forest.\n",
      "\n",
      " Top Recommendation(s) is(are) \n",
      " SOZQVTJ12A6701D96B    bf30441e24ef5326354295723d9fe1edf59b8554,34618...\n",
      "Name: users, dtype: object\n"
     ]
    }
   ],
   "source": [
    "get_recommendations('bf30441e24ef5326354295723d9fe1edf59b8554',m,db, permutations, num_recommendations, forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b8af8",
   "metadata": {},
   "source": [
    "**Troubleshooting**: This implementation is not producing the expected outcome. Given more time, I would do debugging and further investigation. The LSH method is used extensively for document recommendation (with shingling).\n",
    "As a **future extension**, I would try to apply this method to lyrics documents that match with our database data. In this way songs would be recommended based on their lyrics compared to lyrics of songs that the user likes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac87627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0003560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dfeaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try with content based\n",
    "\n",
    "# # predict function need to be changed\n",
    "\n",
    "\n",
    "# # def predict(song_profile, database, perms, num_results, forest): # song_profile in list form\n",
    "    \n",
    "# #     #song_profiles = get_items_from_user(userID, utility_matrix ,database)\n",
    "# #     start_time = time.time()\n",
    "     \n",
    "# #     tokens = preprocess(song_profile)\n",
    "# #     m = MinHash(num_perm=perms)\n",
    "# #     for s in tokens:\n",
    "# #         m.update(s.encode('utf8'))\n",
    "        \n",
    "# #     idx_array = np.array(forest.query(m, num_results*2))\n",
    "# #     if len(idx_array) == 0:\n",
    "# #         return None # if your query is empty, return none\n",
    "    \n",
    "# #     #result = database.iloc[idx_array]['users']\n",
    "# #     result = database.iloc[idx_array]['terms']\n",
    "\n",
    "# #     print('It took %s seconds to query forest.' %(time.time()-start_time))\n",
    "    \n",
    "# #     return result, idx_array\n",
    "\n",
    "# f = pd.read_csv('songs_clean.csv')\n",
    "# # keep only 2 columns : song_id , artist_terms\n",
    "# ff = f.iloc[:,[1,2]]\n",
    "# ff = ff.set_index('song_id')\n",
    "# import re\n",
    "# regex = re.compile(r'[\\\"\\'\\\\\\[\\]]')\n",
    "# ff['artist_terms'] = ff['artist_terms'].apply(lambda x: regex.sub('', x)).astype('string')\n",
    "# ff.loc['SOCIWDW12A8C13D406']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a604b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error\n",
    "#get_recommendations('bf30441e24ef5326354295723d9fe1edf59b8554',m_total,ff, permutations, num_recommendations, forest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
