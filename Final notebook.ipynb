{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d1b4c3",
   "metadata": {},
   "source": [
    "# Computational tools for data science: Comparing Recommendation Systems for MillionSongsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677046f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandranavarrocastillo/opt/anaconda3/lib/python3.9/site-packages/tables/__init__.py:24: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  from .utilsextension import (\n",
      "/Users/alejandranavarrocastillo/opt/anaconda3/lib/python3.9/site-packages/tables/__init__.py:24: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  from .utilsextension import (\n",
      "/Users/alejandranavarrocastillo/opt/anaconda3/lib/python3.9/site-packages/tables/req_versions.py:20: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  min_numpy_version = LooseVersion('1.9.3')\n",
      "/Users/alejandranavarrocastillo/opt/anaconda3/lib/python3.9/site-packages/tables/req_versions.py:21: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  min_numexpr_version = LooseVersion('2.6.2')\n",
      "/Users/alejandranavarrocastillo/opt/anaconda3/lib/python3.9/site-packages/tables/req_versions.py:22: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  min_hdf5_version = LooseVersion('1.8.4')\n",
      "/Users/alejandranavarrocastillo/opt/anaconda3/lib/python3.9/site-packages/tables/req_versions.py:23: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  min_blosc_version = LooseVersion(\"1.4.1\")\n",
      "/Users/alejandranavarrocastillo/opt/anaconda3/lib/python3.9/site-packages/tables/req_versions.py:24: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  min_blosc_bitshuffle_version = LooseVersion(\"1.8.0\")\n",
      "/Users/alejandranavarrocastillo/opt/anaconda3/lib/python3.9/site-packages/tables/filters.py:27: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  blosc_version = LooseVersion(tables.which_lib_version(\"blosc\")[1])\n",
      "/Users/alejandranavarrocastillo/opt/anaconda3/lib/python3.9/site-packages/tables/tests/common.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  hdf5_version = LooseVersion(tables.hdf5_version)\n",
      "/Users/alejandranavarrocastillo/opt/anaconda3/lib/python3.9/site-packages/tables/tests/common.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  blosc_version = LooseVersion(tables.which_lib_version(\"blosc\")[1])\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "import re\n",
    "import statistics\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasketch import MinHash, MinHashLSHForest\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import paired_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm.notebook import tqdm\n",
    "from wordcloud import STOPWORDS, ImageColorGenerator, WordCloud\n",
    "\n",
    "import hdf5_getters as hdf5_getters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1872fe67",
   "metadata": {},
   "source": [
    "## Read the data\n",
    "\n",
    "Assuming that you have all the data in a folder called ```data```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d97be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the songs dataset\n",
    "with ZipFile('data/songs_cleaned.zip', 'r') as zipfile:\n",
    "    data = zipfile.read('out.csv')\n",
    "\n",
    "songs_cleaned = pd.read_csv(BytesIO(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b046169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And pre-process the artist_terms feature\n",
    "\n",
    "def ConvertStringtoList(string):\n",
    "    s = string.replace('[','')\n",
    "    s = s.replace(']','')\n",
    "    s = s.replace('\\'','')\n",
    "    s = s.replace(' ','')\n",
    "    li = list(s.split(\",\"))\n",
    "    return li\n",
    "\n",
    "i = 0\n",
    "for string in songs_cleaned['artist_terms']:\n",
    "    list_ = ConvertStringtoList(string)\n",
    "    songs_cleaned['artist_terms'][i] = list_\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4b61849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_id</th>\n",
       "      <th>song_id</th>\n",
       "      <th>artist_terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARMJAGH1187FB546F3</td>\n",
       "      <td>SOCIWDW12A8C13D406</td>\n",
       "      <td>[blue-eyedsoul, poprock, blues-rock, beachmusi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARXR32B1187FB57099</td>\n",
       "      <td>SOFSOCN12A8C143F5D</td>\n",
       "      <td>[poppunk, skapunk, breakcore, alternativemetal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AR10USD1187B99F3F1</td>\n",
       "      <td>SOHKNRJ12A6701D1F8</td>\n",
       "      <td>[post-hardcore, screamo, emo, hardcore, punkre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARC43071187B990240</td>\n",
       "      <td>SOKEJEJ12A8C13E0D0</td>\n",
       "      <td>[ccm, religiousmusic, losangeles, christianroc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARL7K851187B99ACD2</td>\n",
       "      <td>SOMUYGI12AB0188633</td>\n",
       "      <td>[bachata, merengue, reggaeton, latinpop, spani...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            artist_id             song_id  \\\n",
       "0  ARMJAGH1187FB546F3  SOCIWDW12A8C13D406   \n",
       "1  ARXR32B1187FB57099  SOFSOCN12A8C143F5D   \n",
       "2  AR10USD1187B99F3F1  SOHKNRJ12A6701D1F8   \n",
       "3  ARC43071187B990240  SOKEJEJ12A8C13E0D0   \n",
       "4  ARL7K851187B99ACD2  SOMUYGI12AB0188633   \n",
       "\n",
       "                                        artist_terms  \n",
       "0  [blue-eyedsoul, poprock, blues-rock, beachmusi...  \n",
       "1  [poppunk, skapunk, breakcore, alternativemetal...  \n",
       "2  [post-hardcore, screamo, emo, hardcore, punkre...  \n",
       "3  [ccm, religiousmusic, losangeles, christianroc...  \n",
       "4  [bachata, merengue, reggaeton, latinpop, spani...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf5cacd1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697064\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>songID</th>\n",
       "      <th>play_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>\n",
       "      <td>SOWEZSI12A81C21CE6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4bd88bfb25263a75bbdd467e74018f4ae570e5df</td>\n",
       "      <td>SODCXXY12AB0187452</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b64cdd1a0bd907e5e00b39e345194768e330d652</td>\n",
       "      <td>SOLXDDC12A6701FBFD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b64cdd1a0bd907e5e00b39e345194768e330d652</td>\n",
       "      <td>SONQBUB12A6D4F8ED0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5a905f000fc1ff3df7ca807d57edb608863db05d</td>\n",
       "      <td>SOFKTPP12A8C1385CA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     userID              songID  play_count\n",
       "0  b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOWEZSI12A81C21CE6           1\n",
       "1  4bd88bfb25263a75bbdd467e74018f4ae570e5df  SODCXXY12AB0187452           2\n",
       "2  b64cdd1a0bd907e5e00b39e345194768e330d652  SOLXDDC12A6701FBFD           1\n",
       "3  b64cdd1a0bd907e5e00b39e345194768e330d652  SONQBUB12A6D4F8ED0           2\n",
       "4  5a905f000fc1ff3df7ca807d57edb608863db05d  SOFKTPP12A8C1385CA           1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the user tastes' dataset\n",
    "\n",
    "with ZipFile('users_cleaned.zip', 'r') as zipfile:\n",
    "    data = zipfile.read('out.csv')\n",
    "\n",
    "users_cleaned = pd.read_csv(BytesIO(data))\n",
    "print(len(users_cleaned))\n",
    "users_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "124fc352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 386670 users.\n",
      "In the subset for the evaluation we have a total number of 5000 users.\n"
     ]
    }
   ],
   "source": [
    "# How many different users there are\n",
    "print(\"We have a total of\", len(set(users_cleaned['userID'])), \"users.\")\n",
    "\n",
    "# Let's take a subset of users to evaluate the performance of the various recommendation systems\n",
    "nb_of_users = 5000\n",
    "\n",
    "users_subset = users_cleaned[users_cleaned['userID'].isin(list(set(users_cleaned['userID']))[:nb_of_users])]\n",
    "print(\"In the subset for the evaluation we have a total number of\", len(set(users_subset['userID'])), \"users.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4529fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>songID</th>\n",
       "      <th>play_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>3fd3acaa8dfeb94b0602a33085b44ebe80545dd2</td>\n",
       "      <td>SOBRZCG12A6702187D</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>3fd3acaa8dfeb94b0602a33085b44ebe80545dd2</td>\n",
       "      <td>SONQBUB12A6D4F8ED0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>c231bc806c239b1322421e66fc001822a9b2c2f0</td>\n",
       "      <td>SOBEVGM12A67ADBCA7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>76bcebcaf7b1f20c857bb8a23d0030b086cf292f</td>\n",
       "      <td>SOTEFFR12A8C144765</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>9b392166d01817895c03dc190f4eff58153a25e3</td>\n",
       "      <td>SOCHPTV12A6BD53113</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       userID              songID  play_count\n",
       "226  3fd3acaa8dfeb94b0602a33085b44ebe80545dd2  SOBRZCG12A6702187D           1\n",
       "227  3fd3acaa8dfeb94b0602a33085b44ebe80545dd2  SONQBUB12A6D4F8ED0           1\n",
       "262  c231bc806c239b1322421e66fc001822a9b2c2f0  SOBEVGM12A67ADBCA7           1\n",
       "296  76bcebcaf7b1f20c857bb8a23d0030b086cf292f  SOTEFFR12A8C144765           1\n",
       "330  9b392166d01817895c03dc190f4eff58153a25e3  SOCHPTV12A6BD53113           2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18bfbda",
   "metadata": {},
   "source": [
    "## Content based recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e1644b",
   "metadata": {},
   "source": [
    "### Represent songs as vectors\n",
    "\n",
    "We use the feature artist_terms and implement one-hot-encoding. With one-hot-encoding, we convert each categorical value into a new categorical column and assign a binary value 1 or 0 to each feature if the term was initially in the terms of a song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dce7efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs = songs_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a21e35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs will be represented as binary vectors of dimension 2294\n"
     ]
    }
   ],
   "source": [
    "# First, extract the set of artist_terms\n",
    "\n",
    "all_terms = []\n",
    "for row in range(len(df_songs)):\n",
    "    if df_songs['artist_terms'][row] == []:\n",
    "        print('vacia')\n",
    "    all_terms.append(df_songs['artist_terms'][row])\n",
    "\n",
    "all_terms = np.concatenate(all_terms)\n",
    "all_terms = list(set(all_terms)) # we have gotten a list of the set of all artist_terms\n",
    "\n",
    "d = len(all_terms) # dimension of the vectors we are representing\n",
    "print('Songs will be represented as binary vectors of dimension', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "748a79f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now, we intend to create a binary vector (length = d) that represents a song, \n",
    "# with 1s if the song has this term and 0s if it hasn't.\n",
    "\n",
    "def vectorize(song):\n",
    "\n",
    "    index = int(df_songs.index[df_songs['song_id'] == song][0])\n",
    "    vector = np.zeros(len(all_terms))\n",
    "\n",
    "    for i in range(len(vector)):\n",
    "        if all_terms[i] in df_songs['artist_terms'][index]:\n",
    "            vector[i] = 1\n",
    "            \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7838087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize ALL the songs and save it into a dictionary\n",
    "\n",
    "vector_representation = {}\n",
    "\n",
    "for song in df_songs['song_id']:\n",
    "    vector_representation[song] = vectorize(song)\n",
    "    \n",
    "#vector_representation # we end up with a dictionary of songs with their vector representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d825109",
   "metadata": {},
   "source": [
    "### Get the user profiles\n",
    "\n",
    "We will represent each user as a vector of dimension d. \n",
    "The representation is the weighted average of the songs that the user has already listened to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "751abf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all the results only for the subset of 5000 users\n",
    "user_plays = users_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82b95bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the User profile: that will be a vector of dimension d computed as the weighted average of his played songs\n",
    "# Get user profile for all users\n",
    "\n",
    "def getUserProfile(user):\n",
    "\n",
    "    song_counts = list(user_plays[user_plays['userID'] == user]['play_count'])\n",
    "    song_list = list(user_plays[user_plays['userID'] == user]['songID'])\n",
    "\n",
    "\n",
    "    # Check if the user songs are in the song dataset and get the indices of the songs\n",
    "    indices = []\n",
    "    for song in song_list:\n",
    "        if song in list(df_songs['song_id']):\n",
    "            indices.append(song_list.index(song))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "    # Get the User profile (i.e. Compute the (weighted) average of the songs of a user)\n",
    "    a = [song_counts[i] for i in indices]\n",
    "    b = [vector_representation[song_list[i]] for i in indices]\n",
    "\n",
    "    numerator = np.zeros(d)\n",
    "    for i in range(len(a)):\n",
    "        numerator = numerator + ( a[i] * np.asarray(b[i]) )\n",
    "\n",
    "    user_profile = numerator / sum(a)\n",
    "    return user_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebe0ef81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b5556cc62444c3a7ccdf418719cf09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get user profiles for every user in the users_plays dataset\n",
    "# If users_plays = users_cleaned --> Very slow cell (it lasts 1 day). Set to True to run this cell. Or, read the already created dictionary in the file user_profiles.pkl\n",
    "# Otherwise if users_plays = users_subset, (since it is a subset) OK.\n",
    "\n",
    "create_user_profiles = True\n",
    "\n",
    "if create_user_profiles:\n",
    "    users = set(list(user_plays['userID']))\n",
    "    user_profiles = {}\n",
    "    for user in tqdm(users):\n",
    "        user_profiles[user] = getUserProfile(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06428709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary of user_profiles\n",
    "# Set to True if you want to save the user_profiles dictionary as a separate file\n",
    "if create_user_profiles:\n",
    "    with open(\"user_profiles.pkl\", \"wb\") as f:\n",
    "        pickle.dump(user_profiles, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d68ca28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 5000 users in this dictionary.\n"
     ]
    }
   ],
   "source": [
    "# Read user_profiles file and save it into a dictionary\n",
    "if not create_user_profiles:\n",
    "    with open(\"user_profiles.pkl\", \"rb\") as a_file:\n",
    "        user_profiles = pickle.load(a_file)\n",
    "print(\"We have\", len(user_profiles), \"users in this dictionary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793e4594",
   "metadata": {},
   "source": [
    "### Get recommendation\n",
    "\n",
    "We will recommend the songs of our dataset that are more similar to the user profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "906b5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendations for all users\n",
    "\n",
    "def getRecommendation(user):\n",
    "    user_profile = user_profiles[user]\n",
    "    \n",
    "    scores_dict = {}\n",
    "    for song in df_songs['song_id']:\n",
    "        dist = np.linalg.norm(user_profile - vector_representation[song]) # euclidean distance\n",
    "        scores_dict[song] = dist\n",
    "\n",
    "    # R best recommendations\n",
    "    R = 10\n",
    "    recommended_songs = []\n",
    "    for score in np.sort(list(scores_dict.values()))[0:R]:\n",
    "        recommended_songs.append(list({i for i in scores_dict if scores_dict[i]==score}))\n",
    "\n",
    "    recommended_songs = list(set(np.concatenate(recommended_songs)))\n",
    "\n",
    "    return recommended_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59cb9130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0195d9256b2448883525f3767f9fc78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create dictionary of user_recommendations\n",
    "# Set to True if you want to create it again.\n",
    "# If users_plays = users_cleaned --> Very slow cell (it takes forever)\n",
    "# Otherwise if users_plays = users_subset, OK.\n",
    "if True:\n",
    "    user_recommendations = {}\n",
    "    counter = 0\n",
    "    for key in tqdm(user_profiles.keys()):\n",
    "        if counter < 10000: # control parameter, if we have more than a certain nb of users, don't compute it all\n",
    "            user_recommendations[key] = getRecommendation(key)\n",
    "            counter += 1\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d145b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary of user_recommendations\n",
    "# Set to True if you want to save the user_recommendations dictionary as a separate file\n",
    "save_user_recommendations = True\n",
    "if save_user_recommendations:\n",
    "    with open(\"user_recommendations.pkl\", \"wb\") as f:\n",
    "        pickle.dump(user_recommendations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27b22083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read user_recommendations file and save it into a dictionary\n",
    "if not save_user_recommendations:\n",
    "    with open(\"user_recommendations.pkl\", \"rb\") as a_file:\n",
    "        user_recommendations = pickle.load(a_file)\n",
    "    print(\"We have\", len(user_recommendations), \"users with their recommendations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d17b67",
   "metadata": {},
   "source": [
    "### Evaluation of recommendation\n",
    "\n",
    "We can use different methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eb66b7",
   "metadata": {},
   "source": [
    "#### Intra-list similarity\n",
    "\n",
    "Intra-list similarity is the average cosine similarity of all items in a list of recommendations.\n",
    "Intra-list similarity can be calculated for each user, and averaged over all users in the test set to get an estimate of intra-list similarity for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39bce536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity\n",
    "from numpy.linalg import norm\n",
    "def cosine_similarity(song1, song2):\n",
    "    a = vector_representation[song1]\n",
    "    b = vector_representation[song2]\n",
    "    dist = np.dot(a,b)/(norm(a)*norm(b))\n",
    "    return dist\n",
    "\n",
    "# Intra list similarity function\n",
    "def intra_list_similarity(user):\n",
    "    rec = user_recommendations[user]\n",
    "    \n",
    "    # All possible pairs in list\n",
    "    pairs = [(a, b) for idx, a in enumerate(rec) for b in rec[idx + 1:]]\n",
    "    \n",
    "    # Compute the average distances between the pairs of the recommended songs\n",
    "    distances = []\n",
    "    for pair in pairs:\n",
    "        distances.append(cosine_similarity(pair[0], pair[1]))\n",
    "    \n",
    "    return np.mean(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed0bdd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intra list similarity for Content based model is: 0.5209447271563011\n"
     ]
    }
   ],
   "source": [
    "# Get the intra-list similarity of the model\n",
    "# Average of all intra list similarities\n",
    "\n",
    "intra_list_similar = []\n",
    "for user in user_recommendations.keys():\n",
    "    intra_list_similar.append(intra_list_similarity(user))\n",
    "\n",
    "print(\"The intra list similarity for Content based model is:\" , np.mean(intra_list_similar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76aa158",
   "metadata": {},
   "source": [
    "#### Personalization (in progress)\n",
    "\n",
    "https://towardsdatascience.com/evaluation-metrics-for-recommender-systems-df56c6611093"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b857816",
   "metadata": {},
   "source": [
    "#### Evaluation that I explain in the whatsapp video (Ale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "146ea899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average of the distances of the recommended songs to the user profile \n",
    "# Distance can be calculated by euclidean or cosine similarity\n",
    "\n",
    "def average_distance(user):\n",
    "    rec = user_recommendations[user]\n",
    "    user_profile = user_profiles[user]\n",
    "\n",
    "    distances = []\n",
    "    for song in rec:\n",
    "        a = vector_representation[song]\n",
    "        distances.append(np.linalg.norm(a-user_profile)) # euclidean distance\n",
    "    return np.mean(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06639f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average similarity for Content based model is: 3.120989606575042\n"
     ]
    }
   ],
   "source": [
    "# Get the average similarity of the model\n",
    "# Average of all similarities\n",
    "\n",
    "average_similar = []\n",
    "for user in user_recommendations.keys():\n",
    "    average_similar.append(average_distance(user))\n",
    "\n",
    "print(\"The average similarity for Content based model is:\" , np.mean(average_similar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1668ae77",
   "metadata": {},
   "source": [
    "# Item based collaborative filtering (Angeliki)\n",
    "\n",
    "<sup>Inspired by https://github.com/csaluja/JupyterNotebooks-Medium/blob/master/CF%20Recommendation%20System-Examples.ipynb<sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21475a5b",
   "metadata": {},
   "source": [
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6f80b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a657d40",
   "metadata": {},
   "source": [
    "Read-in the data and create the utility matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a01a26e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(386670, 3195)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the utility matrix\n",
    "utility_matrix = users_cleaned.pivot(index='userID', columns='songID', values='play_count')\n",
    "\n",
    "# Includes 386670 users and 3195 songs\n",
    "utility_matrix.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb3a8848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Narrow down to 10k users and replace the NaN with 0s.\n",
    "\n",
    "df1 = utility_matrix[utility_matrix.index.isin(list(users_subset['userID']))] # the 5000 users subset\n",
    "df2 = utility_matrix[~utility_matrix.index.isin(list(users_subset['userID']))][:5000] # other 5000 subset\n",
    "u1 = pd.concat([df1, df2])\n",
    "np.nan_to_num(u1,copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90a8720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users and songs to experiment with\n",
    "user = '06b4caaf4dcc2476b5ac096f08f4356b6ba9a86a'\n",
    "#user = '00038cf792e9f9a1cb593dea5779f96195aac68c'\n",
    "#user = '0002b896949cb2899feaed47104406e99eafa983'\n",
    "song = 'SOAPNML12A8C13B696'\n",
    "#song = 'SOSHUVD12A6701F8F9'\n",
    "\n",
    "samplelist = list(users_subset['userID'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd010e",
   "metadata": {},
   "source": [
    "**I am implementing item based collaborative filtering as it outperforms user based and items are simpler than user tastes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1fb6be",
   "metadata": {},
   "source": [
    "### Recommend 10 songs to a list of users using adjusted cosine correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c449784",
   "metadata": {},
   "source": [
    "Difference between Pearson's correlation and adjusted cosine correlation:\n",
    "\n",
    "   - In pearson correlation, the mean which subtracted is about the particular item itself (ratings from all users), mean(Ri)\n",
    "   - In adjusted cosine correlation, the mean is about the particular user (ratings to all items), mean(Ru)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "860ebb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes a adjusted cosine correlation matrix from a utility matrix\n",
    "def get_adj_cosine_M(utility_matrix):\n",
    "    M = utility_matrix.to_numpy()\n",
    "    M_u = M.mean(axis=1)\n",
    "    item_mean_subtracted = M - M_u[:, None]\n",
    "    similarity_matrix = 1 - squareform(pdist(item_mean_subtracted.T, 'cosine'))\n",
    "    \n",
    "    return pd.DataFrame(similarity_matrix, index=utility_matrix.columns, columns=utility_matrix.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e5445f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>songID</th>\n",
       "      <th>SOAAAQN12AB01856D3</th>\n",
       "      <th>SOAANKE12A8C13CF5C</th>\n",
       "      <th>SOAASSD12AB0181AA6</th>\n",
       "      <th>SOABLAF12AB018E1D9</th>\n",
       "      <th>SOABRXK12A8C130A36</th>\n",
       "      <th>SOABTKM12A8AE4721E</th>\n",
       "      <th>SOABVPU12AB018AA22</th>\n",
       "      <th>SOABVWD12A58A7C3FF</th>\n",
       "      <th>SOACEDS12A6701EAAA</th>\n",
       "      <th>SOACFRH12A8C13E183</th>\n",
       "      <th>...</th>\n",
       "      <th>SOZWCKB12AB0186C5B</th>\n",
       "      <th>SOZWECJ12A6D4F5229</th>\n",
       "      <th>SOZWVCA12A6D4F9774</th>\n",
       "      <th>SOZXHBQ12AB0186626</th>\n",
       "      <th>SOZXTKD12A8C13FC43</th>\n",
       "      <th>SOZYPNV12A6701E3B8</th>\n",
       "      <th>SOZYZDZ12AB01873CA</th>\n",
       "      <th>SOZZPYH12AB0187578</th>\n",
       "      <th>SOZZQBH12A6D4FAFD8</th>\n",
       "      <th>SOZZVMW12AB0183B52</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>songID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SOAAAQN12AB01856D3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.048477</td>\n",
       "      <td>0.048477</td>\n",
       "      <td>-0.001083</td>\n",
       "      <td>0.003297</td>\n",
       "      <td>-0.001342</td>\n",
       "      <td>0.048477</td>\n",
       "      <td>0.048477</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.048477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004875</td>\n",
       "      <td>0.004948</td>\n",
       "      <td>0.014631</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>0.048477</td>\n",
       "      <td>0.048477</td>\n",
       "      <td>0.048477</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.048477</td>\n",
       "      <td>0.001720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOAANKE12A8C13CF5C</th>\n",
       "      <td>0.048477</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.013425</td>\n",
       "      <td>0.069061</td>\n",
       "      <td>-0.019706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.045263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101682</td>\n",
       "      <td>0.105709</td>\n",
       "      <td>0.302511</td>\n",
       "      <td>0.301665</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009334</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOAASSD12AB0181AA6</th>\n",
       "      <td>0.048477</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.013425</td>\n",
       "      <td>0.069061</td>\n",
       "      <td>-0.019706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.045263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101682</td>\n",
       "      <td>0.105709</td>\n",
       "      <td>0.302511</td>\n",
       "      <td>0.301665</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009334</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOABLAF12AB018E1D9</th>\n",
       "      <td>-0.001083</td>\n",
       "      <td>-0.013425</td>\n",
       "      <td>-0.013425</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001080</td>\n",
       "      <td>-0.000901</td>\n",
       "      <td>-0.013425</td>\n",
       "      <td>-0.013425</td>\n",
       "      <td>-0.001284</td>\n",
       "      <td>-0.013425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001529</td>\n",
       "      <td>-0.001949</td>\n",
       "      <td>-0.004162</td>\n",
       "      <td>-0.004184</td>\n",
       "      <td>-0.013425</td>\n",
       "      <td>-0.013425</td>\n",
       "      <td>-0.013425</td>\n",
       "      <td>-0.001348</td>\n",
       "      <td>-0.013425</td>\n",
       "      <td>-0.001143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOABRXK12A8C130A36</th>\n",
       "      <td>0.003297</td>\n",
       "      <td>0.069061</td>\n",
       "      <td>0.069061</td>\n",
       "      <td>-0.001080</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001498</td>\n",
       "      <td>0.069061</td>\n",
       "      <td>0.069061</td>\n",
       "      <td>0.003046</td>\n",
       "      <td>0.069061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007003</td>\n",
       "      <td>0.007238</td>\n",
       "      <td>0.020880</td>\n",
       "      <td>0.020817</td>\n",
       "      <td>0.069061</td>\n",
       "      <td>0.069061</td>\n",
       "      <td>0.069061</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.069061</td>\n",
       "      <td>0.002668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "songID              SOAAAQN12AB01856D3  SOAANKE12A8C13CF5C  \\\n",
       "songID                                                       \n",
       "SOAAAQN12AB01856D3            1.000000            0.048477   \n",
       "SOAANKE12A8C13CF5C            0.048477            1.000000   \n",
       "SOAASSD12AB0181AA6            0.048477            1.000000   \n",
       "SOABLAF12AB018E1D9           -0.001083           -0.013425   \n",
       "SOABRXK12A8C130A36            0.003297            0.069061   \n",
       "\n",
       "songID              SOAASSD12AB0181AA6  SOABLAF12AB018E1D9  \\\n",
       "songID                                                       \n",
       "SOAAAQN12AB01856D3            0.048477           -0.001083   \n",
       "SOAANKE12A8C13CF5C            1.000000           -0.013425   \n",
       "SOAASSD12AB0181AA6            1.000000           -0.013425   \n",
       "SOABLAF12AB018E1D9           -0.013425            1.000000   \n",
       "SOABRXK12A8C130A36            0.069061           -0.001080   \n",
       "\n",
       "songID              SOABRXK12A8C130A36  SOABTKM12A8AE4721E  \\\n",
       "songID                                                       \n",
       "SOAAAQN12AB01856D3            0.003297           -0.001342   \n",
       "SOAANKE12A8C13CF5C            0.069061           -0.019706   \n",
       "SOAASSD12AB0181AA6            0.069061           -0.019706   \n",
       "SOABLAF12AB018E1D9           -0.001080           -0.000901   \n",
       "SOABRXK12A8C130A36            1.000000           -0.001498   \n",
       "\n",
       "songID              SOABVPU12AB018AA22  SOABVWD12A58A7C3FF  \\\n",
       "songID                                                       \n",
       "SOAAAQN12AB01856D3            0.048477            0.048477   \n",
       "SOAANKE12A8C13CF5C            1.000000            1.000000   \n",
       "SOAASSD12AB0181AA6            1.000000            1.000000   \n",
       "SOABLAF12AB018E1D9           -0.013425           -0.013425   \n",
       "SOABRXK12A8C130A36            0.069061            0.069061   \n",
       "\n",
       "songID              SOACEDS12A6701EAAA  SOACFRH12A8C13E183  ...  \\\n",
       "songID                                                      ...   \n",
       "SOAAAQN12AB01856D3            0.001969            0.048477  ...   \n",
       "SOAANKE12A8C13CF5C            0.045263            1.000000  ...   \n",
       "SOAASSD12AB0181AA6            0.045263            1.000000  ...   \n",
       "SOABLAF12AB018E1D9           -0.001284           -0.013425  ...   \n",
       "SOABRXK12A8C130A36            0.003046            0.069061  ...   \n",
       "\n",
       "songID              SOZWCKB12AB0186C5B  SOZWECJ12A6D4F5229  \\\n",
       "songID                                                       \n",
       "SOAAAQN12AB01856D3            0.004875            0.004948   \n",
       "SOAANKE12A8C13CF5C            0.101682            0.105709   \n",
       "SOAASSD12AB0181AA6            0.101682            0.105709   \n",
       "SOABLAF12AB018E1D9           -0.001529           -0.001949   \n",
       "SOABRXK12A8C130A36            0.007003            0.007238   \n",
       "\n",
       "songID              SOZWVCA12A6D4F9774  SOZXHBQ12AB0186626  \\\n",
       "songID                                                       \n",
       "SOAAAQN12AB01856D3            0.014631            0.014579   \n",
       "SOAANKE12A8C13CF5C            0.302511            0.301665   \n",
       "SOAASSD12AB0181AA6            0.302511            0.301665   \n",
       "SOABLAF12AB018E1D9           -0.004162           -0.004184   \n",
       "SOABRXK12A8C130A36            0.020880            0.020817   \n",
       "\n",
       "songID              SOZXTKD12A8C13FC43  SOZYPNV12A6701E3B8  \\\n",
       "songID                                                       \n",
       "SOAAAQN12AB01856D3            0.048477            0.048477   \n",
       "SOAANKE12A8C13CF5C            1.000000            1.000000   \n",
       "SOAASSD12AB0181AA6            1.000000            1.000000   \n",
       "SOABLAF12AB018E1D9           -0.013425           -0.013425   \n",
       "SOABRXK12A8C130A36            0.069061            0.069061   \n",
       "\n",
       "songID              SOZYZDZ12AB01873CA  SOZZPYH12AB0187578  \\\n",
       "songID                                                       \n",
       "SOAAAQN12AB01856D3            0.048477            0.000046   \n",
       "SOAANKE12A8C13CF5C            1.000000            0.009334   \n",
       "SOAASSD12AB0181AA6            1.000000            0.009334   \n",
       "SOABLAF12AB018E1D9           -0.013425           -0.001348   \n",
       "SOABRXK12A8C130A36            0.069061            0.000501   \n",
       "\n",
       "songID              SOZZQBH12A6D4FAFD8  SOZZVMW12AB0183B52  \n",
       "songID                                                      \n",
       "SOAAAQN12AB01856D3            0.048477            0.001720  \n",
       "SOAANKE12A8C13CF5C            1.000000            0.039670  \n",
       "SOAASSD12AB0181AA6            1.000000            0.039670  \n",
       "SOABLAF12AB018E1D9           -0.013425           -0.001143  \n",
       "SOABRXK12A8C130A36            0.069061            0.002668  \n",
       "\n",
       "[5 rows x 3195 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjcos_sim = get_adj_cosine_M(u1)\n",
    "adjcos_sim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61b9535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds k similar songs given songID and adjusted cosine matrix\n",
    "def get_similar_songs_adjcosine(songID, adj_sim_m , k):\n",
    "    '''Find k similar songs given songID and adjusted cosine matrix '''\n",
    "    \n",
    "    # sort the similarities and grab k highest values\n",
    "    similarities = adj_sim_m [songID].sort_values(ascending=False)[:k+1].values\n",
    "    # grab the songIDs\n",
    "    indices = adj_sim_m [songID].sort_values(ascending=False)[:k+1].index\n",
    "    \n",
    "    print('{} most similar items for item {}:\\n'.format(k,songID))\n",
    "    for i in range(0, len(indices)):\n",
    "            #first index is songID by default\n",
    "            if indices[i] == songID:\n",
    "                continue;\n",
    "\n",
    "            else:\n",
    "                print('{}: Song {} , with similarity of {}'.format(i,indices[i], similarities[i]))\n",
    "        \n",
    "    return similarities ,indices.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bf03221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function recommends 10 songs based on item-item collaborative filtering\n",
    "# given a list of users and a utility matrix (database)\n",
    "def recommend10Items(user_list, database):\n",
    "    result = dict()\n",
    "    # compute adjusted cosine similarity matrix\n",
    "    sim_matrix = get_adj_cosine_M(database)\n",
    "    \n",
    "    for userID in user_list:\n",
    "        # find row corresponding to user in database\n",
    "        idx = database.index.get_loc(userID)\n",
    "        #get the top 5 songs that he already likes\n",
    "        likes = database.iloc[idx, np.argsort(-database.values[idx])[:5]].index\n",
    "        \n",
    "        for songID in likes:\n",
    "            similarities, recommendations = get_similar_songs_adjcosine(songID, sim_matrix, 1)\n",
    "            \n",
    "            if userID not in result:\n",
    "                result[userID] = recommendations\n",
    "            elif type(result[userID]) == list:\n",
    "                result[userID].append(recommendations)\n",
    "            else:\n",
    "                result[userID] = [result[userID], recommendations]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4f21c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 most similar items for item SOBRZCG12A6702187D:\n",
      "\n",
      "1: Song SOZQSGL12AF72A9145 , with similarity of 0.28970648489491746\n",
      "1 most similar items for item SONQBUB12A6D4F8ED0:\n",
      "\n",
      "1: Song SOCHRXB12A8AE48069 , with similarity of 0.25577003422799693\n",
      "1 most similar items for item SOAAAQN12AB01856D3:\n",
      "\n",
      "1: Song SOYJNHO12AB01856DC , with similarity of 0.5735637332552541\n",
      "1 most similar items for item SOQJPYF12AF72AA8E2:\n",
      "\n",
      "1: Song SOHTWLT12A8C13CFE1 , with similarity of 0.13713815953117536\n",
      "1 most similar items for item SOQJWZI12A8C140181:\n",
      "\n",
      "0: Song SOLTAOU12A8C1375CB , with similarity of 1.0\n",
      "1: Song SOLEIOS12AB018372B , with similarity of 1.0\n",
      "1 most similar items for item SOBRZCG12A6702187D:\n",
      "\n",
      "1: Song SOZQSGL12AF72A9145 , with similarity of 0.28970648489491746\n",
      "1 most similar items for item SONQBUB12A6D4F8ED0:\n",
      "\n",
      "1: Song SOCHRXB12A8AE48069 , with similarity of 0.25577003422799693\n",
      "1 most similar items for item SOAAAQN12AB01856D3:\n",
      "\n",
      "1: Song SOYJNHO12AB01856DC , with similarity of 0.5735637332552541\n",
      "1 most similar items for item SOQJPYF12AF72AA8E2:\n",
      "\n",
      "1: Song SOHTWLT12A8C13CFE1 , with similarity of 0.13713815953117536\n",
      "1 most similar items for item SOQJWZI12A8C140181:\n",
      "\n",
      "0: Song SOLTAOU12A8C1375CB , with similarity of 1.0\n",
      "1: Song SOLEIOS12AB018372B , with similarity of 1.0\n",
      "1 most similar items for item SOBEVGM12A67ADBCA7:\n",
      "\n",
      "1: Song SOHPAVE12A8AE47190 , with similarity of 0.21401770049041868\n",
      "1 most similar items for item SOAAAQN12AB01856D3:\n",
      "\n",
      "1: Song SOYJNHO12AB01856DC , with similarity of 0.5735637332552541\n",
      "1 most similar items for item SOQJLFV12AB01897C7:\n",
      "\n",
      "1: Song SOLTAOU12A8C1375CB , with similarity of 0.13968023620175252\n",
      "1 most similar items for item SOQJPYF12AF72AA8E2:\n",
      "\n",
      "1: Song SOHTWLT12A8C13CFE1 , with similarity of 0.13713815953117536\n",
      "1 most similar items for item SOQJWZI12A8C140181:\n",
      "\n",
      "0: Song SOLTAOU12A8C1375CB , with similarity of 1.0\n",
      "1: Song SOLEIOS12AB018372B , with similarity of 1.0\n",
      "1 most similar items for item SOTEFFR12A8C144765:\n",
      "\n",
      "1: Song SOREYUK12A58A7A253 , with similarity of 0.10215251822236326\n",
      "1 most similar items for item SOAAAQN12AB01856D3:\n",
      "\n",
      "1: Song SOYJNHO12AB01856DC , with similarity of 0.5735637332552541\n",
      "1 most similar items for item SOQJLFV12AB01897C7:\n",
      "\n",
      "1: Song SOLTAOU12A8C1375CB , with similarity of 0.13968023620175252\n",
      "1 most similar items for item SOQJPYF12AF72AA8E2:\n",
      "\n",
      "1: Song SOHTWLT12A8C13CFE1 , with similarity of 0.13713815953117536\n",
      "1 most similar items for item SOQJWZI12A8C140181:\n",
      "\n",
      "0: Song SOLTAOU12A8C1375CB , with similarity of 1.0\n",
      "1: Song SOLEIOS12AB018372B , with similarity of 1.0\n",
      "1 most similar items for item SOSDYAS12AB0180457:\n",
      "\n",
      "1: Song SOQHWMN12A6701E2D9 , with similarity of 0.043516577293775116\n",
      "1 most similar items for item SOCHPTV12A6BD53113:\n",
      "\n",
      "1: Song SORXSAJ12A6D4F92BA , with similarity of 0.18351464955569796\n",
      "1 most similar items for item SOQJDEK12A8AE45E7A:\n",
      "\n",
      "0: Song SOLTAOU12A8C1375CB , with similarity of 1.0\n",
      "1: Song SOLEIOS12AB018372B , with similarity of 1.0\n",
      "1 most similar items for item SOQJIWF12A8C13D2F0:\n",
      "\n",
      "0: Song SOLTAOU12A8C1375CB , with similarity of 1.0\n",
      "1: Song SOLEIOS12AB018372B , with similarity of 1.0\n",
      "1 most similar items for item SOQJLFV12AB01897C7:\n",
      "\n",
      "1: Song SOLTAOU12A8C1375CB , with similarity of 0.13968023620175252\n",
      "1 most similar items for item SOSDYAS12AB0180457:\n",
      "\n",
      "1: Song SOQHWMN12A6701E2D9 , with similarity of 0.043516577293775116\n",
      "1 most similar items for item SOCHPTV12A6BD53113:\n",
      "\n",
      "1: Song SORXSAJ12A6D4F92BA , with similarity of 0.18351464955569796\n",
      "1 most similar items for item SOQJDEK12A8AE45E7A:\n",
      "\n",
      "0: Song SOLTAOU12A8C1375CB , with similarity of 1.0\n",
      "1: Song SOLEIOS12AB018372B , with similarity of 1.0\n",
      "1 most similar items for item SOQJIWF12A8C13D2F0:\n",
      "\n",
      "0: Song SOLTAOU12A8C1375CB , with similarity of 1.0\n",
      "1: Song SOLEIOS12AB018372B , with similarity of 1.0\n",
      "1 most similar items for item SOQJLFV12AB01897C7:\n",
      "\n",
      "1: Song SOLTAOU12A8C1375CB , with similarity of 0.13968023620175252\n",
      "1 most similar items for item SOLZOBD12AB0185720:\n",
      "\n",
      "1: Song SOBFTUR12AB0184285 , with similarity of 0.062335033210780266\n",
      "1 most similar items for item SOQJLFV12AB01897C7:\n",
      "\n",
      "1: Song SOLTAOU12A8C1375CB , with similarity of 0.13968023620175252\n",
      "1 most similar items for item SOQJPYF12AF72AA8E2:\n",
      "\n",
      "1: Song SOHTWLT12A8C13CFE1 , with similarity of 0.13713815953117536\n",
      "1 most similar items for item SOQJWZI12A8C140181:\n",
      "\n",
      "0: Song SOLTAOU12A8C1375CB , with similarity of 1.0\n",
      "1: Song SOLEIOS12AB018372B , with similarity of 1.0\n",
      "1 most similar items for item SOQJYCE12A6D4F4844:\n",
      "\n",
      "0: Song SOIKYQE12A81356CFD , with similarity of 1.0\n",
      "1 most similar items for item SOCHRXB12A8AE48069:\n",
      "\n",
      "1: Song SONQBUB12A6D4F8ED0 , with similarity of 0.25577003422799693\n",
      "1 most similar items for item SOAAAQN12AB01856D3:\n",
      "\n",
      "1: Song SOYJNHO12AB01856DC , with similarity of 0.5735637332552541\n",
      "1 most similar items for item SOQJIWF12A8C13D2F0:\n",
      "\n",
      "0: Song SOLTAOU12A8C1375CB , with similarity of 1.0\n",
      "1: Song SOLEIOS12AB018372B , with similarity of 1.0\n",
      "1 most similar items for item SOQJLFV12AB01897C7:\n",
      "\n",
      "1: Song SOLTAOU12A8C1375CB , with similarity of 0.13968023620175252\n",
      "1 most similar items for item SOQJPYF12AF72AA8E2:\n",
      "\n",
      "1: Song SOHTWLT12A8C13CFE1 , with similarity of 0.13713815953117536\n",
      "1 most similar items for item SOITBNC12AF72A0B2A:\n",
      "\n",
      "1: Song SOPEGIG12A6D4F8CAB , with similarity of 0.08081845175060676\n",
      "1 most similar items for item SOIDJZY12A6701DEF7:\n",
      "\n",
      "1: Song SOMHOLR12A6D4FBE6A , with similarity of 0.10811162816177222\n",
      "1 most similar items for item SOQJIWF12A8C13D2F0:\n",
      "\n",
      "0: Song SOLTAOU12A8C1375CB , with similarity of 1.0\n",
      "1: Song SOLEIOS12AB018372B , with similarity of 1.0\n",
      "1 most similar items for item SOQJLFV12AB01897C7:\n",
      "\n",
      "1: Song SOLTAOU12A8C1375CB , with similarity of 0.13968023620175252\n",
      "1 most similar items for item SOQJPYF12AF72AA8E2:\n",
      "\n",
      "1: Song SOHTWLT12A8C13CFE1 , with similarity of 0.13713815953117536\n",
      "1 most similar items for item SOITBNC12AF72A0B2A:\n",
      "\n",
      "1: Song SOPEGIG12A6D4F8CAB , with similarity of 0.08081845175060676\n",
      "1 most similar items for item SOIDJZY12A6701DEF7:\n",
      "\n",
      "1: Song SOMHOLR12A6D4FBE6A , with similarity of 0.10811162816177222\n",
      "1 most similar items for item SOQJIWF12A8C13D2F0:\n",
      "\n",
      "0: Song SOLTAOU12A8C1375CB , with similarity of 1.0\n",
      "1: Song SOLEIOS12AB018372B , with similarity of 1.0\n",
      "1 most similar items for item SOQJLFV12AB01897C7:\n",
      "\n",
      "1: Song SOLTAOU12A8C1375CB , with similarity of 0.13968023620175252\n",
      "1 most similar items for item SOQJPYF12AF72AA8E2:\n",
      "\n",
      "1: Song SOHTWLT12A8C13CFE1 , with similarity of 0.13713815953117536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'3fd3acaa8dfeb94b0602a33085b44ebe80545dd2': ['SOBRZCG12A6702187D',\n",
       "  'SOZQSGL12AF72A9145',\n",
       "  'SONQBUB12A6D4F8ED0',\n",
       "  'SOCHRXB12A8AE48069',\n",
       "  'SOAAAQN12AB01856D3',\n",
       "  'SOYJNHO12AB01856DC',\n",
       "  'SOQJPYF12AF72AA8E2',\n",
       "  'SOHTWLT12A8C13CFE1',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOLEIOS12AB018372B',\n",
       "  'SOBRZCG12A6702187D',\n",
       "  'SOZQSGL12AF72A9145',\n",
       "  'SONQBUB12A6D4F8ED0',\n",
       "  'SOCHRXB12A8AE48069',\n",
       "  'SOAAAQN12AB01856D3',\n",
       "  'SOYJNHO12AB01856DC',\n",
       "  'SOQJPYF12AF72AA8E2',\n",
       "  'SOHTWLT12A8C13CFE1',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOLEIOS12AB018372B'],\n",
       " 'c231bc806c239b1322421e66fc001822a9b2c2f0': ['SOBEVGM12A67ADBCA7',\n",
       "  'SOHPAVE12A8AE47190',\n",
       "  'SOAAAQN12AB01856D3',\n",
       "  'SOYJNHO12AB01856DC',\n",
       "  'SOQJLFV12AB01897C7',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOQJPYF12AF72AA8E2',\n",
       "  'SOHTWLT12A8C13CFE1',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOLEIOS12AB018372B'],\n",
       " '76bcebcaf7b1f20c857bb8a23d0030b086cf292f': ['SOTEFFR12A8C144765',\n",
       "  'SOREYUK12A58A7A253',\n",
       "  'SOAAAQN12AB01856D3',\n",
       "  'SOYJNHO12AB01856DC',\n",
       "  'SOQJLFV12AB01897C7',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOQJPYF12AF72AA8E2',\n",
       "  'SOHTWLT12A8C13CFE1',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOLEIOS12AB018372B'],\n",
       " '9b392166d01817895c03dc190f4eff58153a25e3': ['SOSDYAS12AB0180457',\n",
       "  'SOQHWMN12A6701E2D9',\n",
       "  'SOCHPTV12A6BD53113',\n",
       "  'SORXSAJ12A6D4F92BA',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOLEIOS12AB018372B',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOLEIOS12AB018372B',\n",
       "  'SOQJLFV12AB01897C7',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOSDYAS12AB0180457',\n",
       "  'SOQHWMN12A6701E2D9',\n",
       "  'SOCHPTV12A6BD53113',\n",
       "  'SORXSAJ12A6D4F92BA',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOLEIOS12AB018372B',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOLEIOS12AB018372B',\n",
       "  'SOQJLFV12AB01897C7',\n",
       "  'SOLTAOU12A8C1375CB'],\n",
       " '37a8e0d8e4fc8cc03be56671e063af20237d6019': ['SOLZOBD12AB0185720',\n",
       "  'SOBFTUR12AB0184285',\n",
       "  'SOQJLFV12AB01897C7',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOQJPYF12AF72AA8E2',\n",
       "  'SOHTWLT12A8C13CFE1',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOLEIOS12AB018372B',\n",
       "  'SOIKYQE12A81356CFD',\n",
       "  'SOQJYCE12A6D4F4844'],\n",
       " 'dac821fa75a40a9c1dd845864b120cda076f3813': ['SOCHRXB12A8AE48069',\n",
       "  'SONQBUB12A6D4F8ED0',\n",
       "  'SOAAAQN12AB01856D3',\n",
       "  'SOYJNHO12AB01856DC',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOLEIOS12AB018372B',\n",
       "  'SOQJLFV12AB01897C7',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOQJPYF12AF72AA8E2',\n",
       "  'SOHTWLT12A8C13CFE1'],\n",
       " 'ec1e5cff98929e10cd8f9c70a94ccb2001c31ce6': ['SOITBNC12AF72A0B2A',\n",
       "  'SOPEGIG12A6D4F8CAB',\n",
       "  'SOIDJZY12A6701DEF7',\n",
       "  'SOMHOLR12A6D4FBE6A',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOLEIOS12AB018372B',\n",
       "  'SOQJLFV12AB01897C7',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOQJPYF12AF72AA8E2',\n",
       "  'SOHTWLT12A8C13CFE1',\n",
       "  'SOITBNC12AF72A0B2A',\n",
       "  'SOPEGIG12A6D4F8CAB',\n",
       "  'SOIDJZY12A6701DEF7',\n",
       "  'SOMHOLR12A6D4FBE6A',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOLEIOS12AB018372B',\n",
       "  'SOQJLFV12AB01897C7',\n",
       "  'SOLTAOU12A8C1375CB',\n",
       "  'SOQJPYF12AF72AA8E2',\n",
       "  'SOHTWLT12A8C13CFE1']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recom = recommend10Items(samplelist, u1)\n",
    "\n",
    "# flatten the recommendation list\n",
    "from pandas.core.common import flatten\n",
    "\n",
    "for user, rec in get_recom.items():\n",
    "    get_recom[user] = list(flatten(rec))\n",
    "\n",
    "user_recommendations = get_recom\n",
    "user_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9078b7",
   "metadata": {},
   "source": [
    "#### Intra-list similarity\n",
    "\n",
    "Intra-list similarity is the average cosine similarity of all items in a list of recommendations.\n",
    "Intra-list similarity can be calculated for each user, and averaged over all users in the test set to get an estimate of intra-list similarity for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdbab404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intra list similarity for Item based collaborative filtering model is: 0.21325619488889447\n"
     ]
    }
   ],
   "source": [
    "# Get the intra-list similarity of the model\n",
    "# Average of all intra list similarities\n",
    "\n",
    "intra_list_similar = []\n",
    "for user in user_recommendations.keys():\n",
    "    intra_list_similar.append(intra_list_similarity(user))\n",
    "\n",
    "print(\"The intra list similarity for Item based collaborative filtering model is:\" ,np.mean(intra_list_similar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3684fef7",
   "metadata": {},
   "source": [
    "#### Evaluation that I explain in the whatsapp video (Ale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "726c2d68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average similarity for Item based collaborative filtering model is: 5.660672626625526\n"
     ]
    }
   ],
   "source": [
    "# Get the average similarity of the model\n",
    "# Average of all similarities\n",
    "\n",
    "average_similar = []\n",
    "for user in user_recommendations.keys():\n",
    "    average_similar.append(average_distance(user))\n",
    "\n",
    "print(\"The average similarity for Item based collaborative filtering model is:\" , np.mean(average_similar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f52512",
   "metadata": {},
   "source": [
    "### Recommend 10 songs to a list of users by implementig kNN search for item based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec7a2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_songs_kNN(songID, user_profiles, similarity_metric , k):\n",
    "    '''Find k most similar songs to a given songID'''\n",
    "    similarity = list()\n",
    "    neigh_ind = list()\n",
    "    song_profiles=user_profiles.T\n",
    "    \n",
    "    knn = NearestNeighbors(metric = similarity_metric , algorithm = 'brute')\n",
    "    knn.fit(song_profiles.values) #taking .values to avoid sklearn warning\n",
    "                                #UserWarning: X does not have valid feature names, but NearestNeighbors was fitted with feature names\n",
    "    \n",
    "    neigh_dist, neigh_ind = knn.kneighbors(song_profiles.loc[songID].values.reshape(1,-1), n_neighbors = k+1) #plus one, bcs it includes the user we want to compare against \n",
    "    similarity = 1-neigh_dist.flatten()\n",
    "    \n",
    "    similar_songs = []\n",
    "    for i in range(0,len(neigh_ind.flatten())):\n",
    "        if song_profiles.index[neigh_ind.flatten()[i]] == songID:\n",
    "            continue;\n",
    "        else:\n",
    "            similar_songs.append(song_profiles.index[neigh_ind.flatten()[i]])\n",
    "            \n",
    "    return similar_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "558ba0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend10Items_kNN(user_list, database):\n",
    "    result = dict()\n",
    "    \n",
    "    for userID in user_list:\n",
    "        # find row corresponding to user in database\n",
    "        idx = database.index.get_loc(userID)\n",
    "        #get the top 5 songs that he already likes\n",
    "        likes = database.iloc[idx, np.argsort(-database.values[idx])[:5]].index\n",
    "        \n",
    "        for songID in likes:\n",
    "            simsongs= get_similar_songs_kNN(songID, database, 'cosine', 2)\n",
    "            \n",
    "            if userID not in result:\n",
    "                result[userID] = simsongs\n",
    "            elif type(result[userID]) == list:\n",
    "                result[userID].append(simsongs)\n",
    "            else:\n",
    "                result[userID] = [result[userID], simsongs]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8a50508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3fd3acaa8dfeb94b0602a33085b44ebe80545dd2': ['SOZQSGL12AF72A9145',\n",
       "  'SOBSEGK12A58A7BEBF',\n",
       "  'SOCHRXB12A8AE48069',\n",
       "  'SOIHJSD12A6701EB04',\n",
       "  'SOYJNHO12AB01856DC',\n",
       "  'SOSPNDI12AB017F769',\n",
       "  'SOHTWLT12A8C13CFE1',\n",
       "  'SOQFUXL12A8C136D6B',\n",
       "  'SOAAAQN12AB01856D3',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6',\n",
       "  'SOZQSGL12AF72A9145',\n",
       "  'SOBSEGK12A58A7BEBF',\n",
       "  'SOCHRXB12A8AE48069',\n",
       "  'SOIHJSD12A6701EB04',\n",
       "  'SOYJNHO12AB01856DC',\n",
       "  'SOSPNDI12AB017F769',\n",
       "  'SOHTWLT12A8C13CFE1',\n",
       "  'SOQFUXL12A8C136D6B',\n",
       "  'SOAAAQN12AB01856D3',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6'],\n",
       " 'c231bc806c239b1322421e66fc001822a9b2c2f0': ['SOHPAVE12A8AE47190',\n",
       "  'SOKXYUW12A8C140229',\n",
       "  'SOYJNHO12AB01856DC',\n",
       "  'SOSPNDI12AB017F769',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6',\n",
       "  'SOHTWLT12A8C13CFE1',\n",
       "  'SOQFUXL12A8C136D6B',\n",
       "  'SOAAAQN12AB01856D3',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6'],\n",
       " '76bcebcaf7b1f20c857bb8a23d0030b086cf292f': ['SOREYUK12A58A7A253',\n",
       "  'SOTPQFM12AB017AC9E',\n",
       "  'SOYJNHO12AB01856DC',\n",
       "  'SOSPNDI12AB017F769',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6',\n",
       "  'SOHTWLT12A8C13CFE1',\n",
       "  'SOQFUXL12A8C136D6B',\n",
       "  'SOAAAQN12AB01856D3',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6'],\n",
       " '9b392166d01817895c03dc190f4eff58153a25e3': ['SOQHWMN12A6701E2D9',\n",
       "  'SOZVZWP12A58A7BAD1',\n",
       "  'SORXSAJ12A6D4F92BA',\n",
       "  'SOQHTSH12AB01849EE',\n",
       "  'SOAAAQN12AB01856D3',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6',\n",
       "  'SOAAAQN12AB01856D3',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6',\n",
       "  'SOQHWMN12A6701E2D9',\n",
       "  'SOZVZWP12A58A7BAD1',\n",
       "  'SORXSAJ12A6D4F92BA',\n",
       "  'SOQHTSH12AB01849EE',\n",
       "  'SOAAAQN12AB01856D3',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6',\n",
       "  'SOAAAQN12AB01856D3',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6'],\n",
       " '37a8e0d8e4fc8cc03be56671e063af20237d6019': ['SOBFTUR12AB0184285',\n",
       "  'SOBEVGM12A67ADBCA7',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6',\n",
       "  'SOHTWLT12A8C13CFE1',\n",
       "  'SOQFUXL12A8C136D6B',\n",
       "  'SOAAAQN12AB01856D3',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6',\n",
       "  'SOIKYQE12A81356CFD',\n",
       "  'SOMOHHA12A81356D12'],\n",
       " 'dac821fa75a40a9c1dd845864b120cda076f3813': ['SONQBUB12A6D4F8ED0',\n",
       "  'SOKNFFM12A81C216F3',\n",
       "  'SOYJNHO12AB01856DC',\n",
       "  'SOSPNDI12AB017F769',\n",
       "  'SOAAAQN12AB01856D3',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6',\n",
       "  'SOHTWLT12A8C13CFE1',\n",
       "  'SOQFUXL12A8C136D6B'],\n",
       " 'ec1e5cff98929e10cd8f9c70a94ccb2001c31ce6': ['SOPEGIG12A6D4F8CAB',\n",
       "  'SOIDJZY12A6701DEF7',\n",
       "  'SOMHOLR12A6D4FBE6A',\n",
       "  'SOEKVHT12AB018D3D0',\n",
       "  'SOAAAQN12AB01856D3',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6',\n",
       "  'SOHTWLT12A8C13CFE1',\n",
       "  'SOQFUXL12A8C136D6B',\n",
       "  'SOPEGIG12A6D4F8CAB',\n",
       "  'SOIDJZY12A6701DEF7',\n",
       "  'SOMHOLR12A6D4FBE6A',\n",
       "  'SOEKVHT12AB018D3D0',\n",
       "  'SOAAAQN12AB01856D3',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6',\n",
       "  'SOAANKE12A8C13CF5C',\n",
       "  'SOAASSD12AB0181AA6',\n",
       "  'SOHTWLT12A8C13CFE1',\n",
       "  'SOQFUXL12A8C136D6B']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recom_kNN = recommend10Items_kNN(samplelist, u1)\n",
    "get_recom_kNN\n",
    "\n",
    "# flatten the recommendation list\n",
    "for user, rec in get_recom_kNN.items():\n",
    "    get_recom_kNN[user] = list(flatten(rec))\n",
    "\n",
    "user_recommendations = get_recom_kNN\n",
    "user_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6c3c5",
   "metadata": {},
   "source": [
    "#### Intra-list similarity\n",
    "\n",
    "Intra-list similarity is the average cosine similarity of all items in a list of recommendations.\n",
    "Intra-list similarity can be calculated for each user, and averaged over all users in the test set to get an estimate of intra-list similarity for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f025e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intra list similarity for Item based collaborative filtering by implementing kNN model is: 0.13883288337015995\n"
     ]
    }
   ],
   "source": [
    "# Get the intra-list similarity of the model\n",
    "# Average of all intra list similarities\n",
    "\n",
    "intra_list_similar = []\n",
    "for user in user_recommendations.keys():\n",
    "    intra_list_similar.append(intra_list_similarity(user))\n",
    "\n",
    "print(\"The intra list similarity for Item based collaborative filtering by implementing kNN model is:\" ,np.mean(intra_list_similar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b0caf",
   "metadata": {},
   "source": [
    "#### Evaluation that I explain in the whatsapp video (Ale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfcf1485",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average similarity for Item based collaborative filtering by implementing kNN model is: 6.164115742941303\n"
     ]
    }
   ],
   "source": [
    "# Get the average similarity of the model\n",
    "# Average of all similarities\n",
    "\n",
    "average_similar = []\n",
    "for user in user_recommendations.keys():\n",
    "    average_similar.append(average_distance(user))\n",
    "\n",
    "print(\"The average similarity for Item based collaborative filtering by implementing kNN model is:\" , np.mean(average_similar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11053d47",
   "metadata": {},
   "source": [
    "# User-user Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cf1753",
   "metadata": {},
   "source": [
    "The original file contains\n",
    "- 1,019,318 unique users\n",
    "- 48,373,586 user-song.play count triplets\n",
    "\n",
    "A subset of 50000 triplets can be found in triplets_50000.txt, where each line is in the format:\n",
    "    \n",
    "    userID \\tab songID \\tab play_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5c5a0",
   "metadata": {},
   "source": [
    "Read in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profiles = pd.read_csv('data/triplets_1000.txt', sep='\\t', names = ['userID','songID', 'play_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b344c36f",
   "metadata": {},
   "source": [
    "The problem: the original dataset of triplets is too large to be converted in this way.\n",
    "Possible solutions:\n",
    "1. dtype optimization\n",
    "2. Split data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd1f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9762f054",
   "metadata": {},
   "source": [
    "Pivot to tranform the data from long to wide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d607fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profiles = user_profiles.pivot(index='userID', columns='songID', values='play_count')\n",
    "user_profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d3fa46",
   "metadata": {},
   "source": [
    "Drop the columns where all elements are NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profiles = user_profiles.dropna(axis=1, how='all') #doesn't make sense, a song will only exist if a user has listend to it\n",
    "user_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39849030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the NaN with 0s.\n",
    "user_profiles = user_profiles.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd1321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save it as a csv (do it only once)\n",
    "#user_profiles.to_csv(path_or_buf= 'user_profile_from_50000_triplets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de43bed5",
   "metadata": {},
   "source": [
    "Get **cosine similarity** for play counts between users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise_distances is the distance between counts, thus 1 - pairwise_distances is the similarity between counts\n",
    "cosine_sim = 1-pairwise_distances(user_profiles , metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58753870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cosine similarity matrix for the users\n",
    "M_cosine = pd.DataFrame(cosine_sim)\n",
    "M_cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f229e10",
   "metadata": {},
   "source": [
    "Get **pearson similarity** for all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_sim = 1-pairwise_distances(user_profiles, metric=\"correlation\")\n",
    "M_pearson = pd.DataFrame(pearson_sim)\n",
    "M_pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c33279",
   "metadata": {},
   "source": [
    "Same for euclidean and hamming :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0268837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_sim = 1-pairwise_distances(user_profiles, metric=\"euclidean\")\n",
    "M_euclidean = pd.DataFrame(euclidean_sim)\n",
    "\n",
    "hamming_sim = 1-pairwise_distances(user_profiles, metric=\"hamming\")\n",
    "M_hamming = pd.DataFrame(hamming_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d55412",
   "metadata": {},
   "source": [
    "## Find k similar users to a given user\n",
    "\n",
    "A function that finds k similar users given userID and the user_profiles matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5864cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarusers(userID, user_profiles, similarity_metric , k):\n",
    "    '''Find k most similar users to a given userID'''\n",
    "    similarity = list()\n",
    "    neigh_ind = list()\n",
    "    \n",
    "    knn = NearestNeighbors(metric = similarity_metric , algorithm = 'brute')\n",
    "    knn.fit(user_profiles.values) #taking .values to avoid sklearn warning\n",
    "                                #UserWarning: X does not have valid feature names, but NearestNeighbors was fitted with feature names\n",
    "    \n",
    "    neigh_dist, neigh_ind = knn.kneighbors(user_profiles.loc[userID].values.reshape(1,-1), n_neighbors = k+1) #plus one, bcs it includes the user we want to compare against \n",
    "    similarity = 1-neigh_dist.flatten()\n",
    "    print('{} most similar users to user {}, using {} similarity:\\n'.format(k, userID, similarity_metric))\n",
    "    \n",
    "    for i in range(0,len(neigh_ind.flatten())):\n",
    "        if user_profiles.index[neigh_ind.flatten()[i]] == userID:\n",
    "            continue;\n",
    "        else:\n",
    "            print('{}: User {}, with similarity of {}'.format(i, user_profiles.index[neigh_ind.flatten()[i]], similarity.flatten()[i]))\n",
    "            \n",
    "    return similarity,neigh_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c2aaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using cosine similarity\n",
    "similarities,indices = get_similarusers( '5a905f000fc1ff3df7ca807d57edb608863db05d', user_profiles , similarity_metric = 'cosine', k = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac13c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using correlation similarity\n",
    "similarities,indices = get_similarusers( '5a905f000fc1ff3df7ca807d57edb608863db05d', user_profiles , similarity_metric = 'correlation', k = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1baafda",
   "metadata": {},
   "source": [
    "## Predict play count for a user-song combination based on user-user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8406c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_play_count_uu(userID, songID, user_profiles, similarity_metric, k):\n",
    "    '''Predict play count for a particular user-song tuple, based on user-to-user similarity. Use with cosine similarity.'''\n",
    "    prediction = 0\n",
    "    similarity, indices = get_similarusers(userID, user_profiles, similarity_metric, k) #similar users based on cosine similarity\n",
    "    # get mean play count for a user, to adjust\n",
    "\n",
    "    mean_play_count = user_profiles.loc[userID, :].mean() \n",
    "    # weight_i is the similarity of neigbhor_i to user X\n",
    "    sum_of_similarity = np.sum(similarity) - 1 # -1 because user 1 is included, has a similarity of 1\n",
    "    \n",
    "    # initializing variables\n",
    "    w_similarity = 1\n",
    "    weighted_sum = 0\n",
    "    \n",
    "    for i in range(0, len(indices.flatten())):\n",
    "        if user_profiles.index[indices.flatten()[i]] == userID:\n",
    "            continue;\n",
    "        else:\n",
    "            # Normalize ratings for a given user by subtracting row mean (centered cosine, or pearson cor)\n",
    "            try:\n",
    "                songidx = user_profiles.columns.get_loc(songID)\n",
    "            except KeyError:\n",
    "                print(f'Warning: song {songID} not found for user {user_profiles.index[indices.flatten()[i]]}')\n",
    "                continue\n",
    "            try:\n",
    "                person = indices.flatten()[i]\n",
    "            except KeyError:\n",
    "                print(f'Warning: user {person} not found')\n",
    "                continue\n",
    "            play_count_dif = user_profiles.iloc[person, songidx] - np.mean(user_profiles.iloc[indices.flatten()[i],:])\n",
    "               \n",
    "            w_similarity = play_count_dif*similarity[i]\n",
    "            weighted_sum += w_similarity\n",
    "            \n",
    "    prediction = mean_play_count + (weighted_sum/sum_of_similarity)\n",
    "    print('Predicted rating for user {} -> song {}: {}'.format(userID, songID, prediction))\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392034ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_play_count_uu('5a905f000fc1ff3df7ca807d57edb608863db05d', 'SOZZYAO12A6701FF36', user_profiles, 'cosine', 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46880bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out low play songs to get better results?\n",
    "sum_col = user_profiles.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e33b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(sum_col))\n",
    "print(min(sum_col))\n",
    "print(statistics.median(sum_col))\n",
    "print(statistics.mean(sum_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many columns have a sum of 2 or less play counts\n",
    "ignore_indexes, count= [], 0\n",
    "for i, value in enumerate(sum_col):\n",
    "    if value <= 2:\n",
    "        count += 1\n",
    "        ignore_indexes.append(i)\n",
    "print(count)\n",
    "print(len(ignore_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e3ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns based on index list created above\n",
    "df2 = user_profiles.drop(user_profiles.iloc[:, ignore_indexes], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be77528",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df620fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "27898 - 16601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516202bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4750236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now repeat prediction for user ff4322e94814d3c7895d07e6f94139b092862611 and song SOAADCB12A81C22AFA\n",
    "predict_play_count_uu('b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOAADCB12A81C22AFA', df2, 'cosine', 10 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43989194",
   "metadata": {},
   "source": [
    "# Item-Item collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b2e57",
   "metadata": {},
   "source": [
    "Read in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9181572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sparse preprocessed pandas dataframe if available \n",
    "if os.path.exists('data/df.pkl'): \n",
    "    with open('data/df.pkl', 'rb') as f:\n",
    "        user_profiles = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'user_profiles' in globals():\n",
    "    frame = pd.read_csv('triplets_1000.txt', sep='\\t', names = ['userID','songID', 'play_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd86f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'user_profiles' in globals():\n",
    "    person_u = list(frame.userID.unique())\n",
    "    thing_u = list(frame.songID.unique())\n",
    "\n",
    "    data = frame['play_count'].tolist()\n",
    "    row = frame.userID.astype('category').cat.codes\n",
    "    col = frame.songID.astype('category').cat.codes\n",
    "    sparse_matrix = csr_matrix((data, (row, col)), shape=(len(person_u), len(thing_u)))\n",
    "    user_profiles = pd.DataFrame.sparse.from_spmatrix(sparse_matrix, index=person_u, columns=thing_u)\n",
    "user_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7659d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/df.pkl'): \n",
    "    with open('data/df.pkl', 'wb') as f:\n",
    "        pickle.dump(user_profiles, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5b90c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://millionsongdataset.com/sites/default/files/AdditionalFiles/unique_tracks.txt\n",
    "songs = pd.read_csv('data/unique_tracks.txt' ,sep='<SEP>', names=['track_id',  'song_id',  'artist_name', 'song_title'], engine='python') \n",
    "songs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0dbf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_song_name = defaultdict(lambda : 'NA', zip(songs.song_id, songs.song_title))\n",
    "unique_songs = user_profiles.columns\n",
    "names = []\n",
    "for song in unique_songs:\n",
    "    names.append(id_to_song_name[song]) \n",
    "# check number of missing songs\n",
    "a = np.array(names)\n",
    "a[a == 'NA'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78efecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_profiles = user_profiles.T\n",
    "# song_profiles = song_profiles.fillna(0)\n",
    "song_profiles = song_profiles#.sparse.to_dense()\n",
    "song_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198d22eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_songs(songID, song_profiles, similarity_metric, k):\n",
    "    '''Find k most similar users to a given userID'''\n",
    "    similarity = list()\n",
    "    neigh_ind = list()\n",
    "    \n",
    "    knn = NearestNeighbors(metric = similarity_metric, algorithm = 'brute')\n",
    "    knn.fit(song_profiles.values) #taking .values to avoid sklearn warning\n",
    "                                #UserWarning: X does not have valid feature names, but NearestNeighbors was fitted with feature names\n",
    "    \n",
    "    neigh_dist, neigh_ind = knn.kneighbors(song_profiles.loc[songID].values.reshape(1,-1), n_neighbors = k+1) #plus one, bcs it includes the user we want to compare against \n",
    "    similarity = 1-neigh_dist.flatten()\n",
    "    print('{} most similar song to song {}, using {} similarity:\\n'.format(k, id_to_song_name[songID], similarity_metric))\n",
    "    \n",
    "    for i in range(0,len(neigh_ind.flatten())):\n",
    "        song_id = song_profiles.index[neigh_ind.flatten()[i]]\n",
    "        if song_id == songID:\n",
    "            continue;\n",
    "        else:\n",
    "            print('{}: song {}, with similarity of {}'.format(i, id_to_song_name[song_id], similarity.flatten()[i]))\n",
    "            \n",
    "    return similarity, neigh_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7138fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "song = 'SOAARXR12A8C133D15'\n",
    "print('song name: ', id_to_song_name[song])\n",
    "similarities,indices = get_similar_songs(song, song_profiles, similarity_metric = 'correlation', k = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a91e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_recommendation(song, data, n):\n",
    "    '''Randomly recommend n songs to a user'''\n",
    "    # get a list of all the songs\n",
    "    all_songs = np.array(data.index)\n",
    "    # randomly sample n songs\n",
    "    random_song_ids = np.random.randint(0, len(all_songs), n)\n",
    "    rec_songs = all_songs[random_song_ids]\n",
    "    query_songidx = np.where(data.index == song)[0][0]\n",
    "    sims = []\n",
    "    for i in range(len(rec_songs)):\n",
    "        # maybe a problem that it uses the cosine similarity, but pearson is not implemented for paired distances in this way\n",
    "        sim = 1-paired_distances(np.array(data.iloc[query_songidx,:]).reshape(1, -1), np.array(data.iloc[random_song_ids[i],:]).reshape(1, -1), method='cosine')\n",
    "        sims.append(sim)\n",
    "    song_id = song_profiles.index[random_song_ids.flatten()]\n",
    "    rec_songs = [(id_to_song_name[song_id[i]], sims[i][0]) for i in range(len(sims))]\n",
    "    return rec_songs\n",
    "song = 'SOAARXR12A8C133D15'\n",
    "print('query',id_to_song_name[song])\n",
    "random_recommendation(song, song_profiles, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d409fb0",
   "metadata": {},
   "source": [
    "# Locality Sensitive hashing in Collaborative item-item filtering (inspired by week5 : SimilarItems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21338a8f",
   "metadata": {},
   "source": [
    "Explain:\n",
    "- LSH vs KNN, and why LSH is more efficient\n",
    "- Present a scheme of how LSH algorithm operates\n",
    "- Why we use Jaccard distance here (and how we make the decision for binary outcome, \"likes\":1/\"dislikes\":0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33111787",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ Alejandra's ############################################\n",
    "if not os.path.exists('data/MillionSongSubset.pkl'):\n",
    "    path = 'MillionSongSubset'\n",
    "    songs_list = []\n",
    "\n",
    "    for (root, dirs, file) in os.walk(path):\n",
    "        for f in file:\n",
    "            songs_list.append(os.path.dirname(f))\n",
    "    ## READ DATA PATH FROM FILE\n",
    "    songs_file_paths = []\n",
    "\n",
    "    for root, dirs, files in os.walk(os.path.abspath(path)):\n",
    "        for file in files:        \n",
    "            strs = os.path.join(root, file)\n",
    "            new_strs = strs.replace('\\\\','/')\n",
    "            songs_file_paths.append(new_strs)\n",
    "            \n",
    "    ### CREATE PANDAS TABLE\n",
    "\n",
    "    N = len(songs_file_paths)\n",
    "    data = []\n",
    "\n",
    "    for i in tqdm(range(N)):\n",
    "        record = []\n",
    "        \n",
    "        # Open specific song path\n",
    "        h5 = hdf5_getters.open_h5_file_read(songs_file_paths[i])\n",
    "        artist_id = hdf5_getters.get_artist_id(h5)\n",
    "        artist_id = artist_id.decode(\"utf-8\")   \n",
    "        \n",
    "        song_id = hdf5_getters.get_song_id(h5)\n",
    "        song_id = song_id.decode(\"utf-8\")\n",
    "        \n",
    "        song_name = hdf5_getters.get_title(h5)\n",
    "        song_name = song_name.decode(\"utf-8\")\n",
    "        artist_terms_ = hdf5_getters.get_artist_terms(h5)\n",
    "        artist_terms = []\n",
    "        for j in range(len(artist_terms_)):\n",
    "            artist_terms.append(artist_terms_[j].decode(\"utf-8\"))\n",
    "        \n",
    "        # Close file\n",
    "        h5.close()\n",
    "        record.append(artist_id)\n",
    "        record.append(song_id)\n",
    "        record.append(song_name)\n",
    "        record.append(artist_terms)\n",
    "        data.append(record)\n",
    "        \n",
    "    df = pd.DataFrame(data, columns=['artist_id','song_id','song_name','artist_terms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6bb27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/MillionSongSubset.pkl'):\n",
    "    with open('data/MillionSongSubset.pkl', 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "else:\n",
    "    with open('data/MillionSongSubset.pkl', 'rb') as f:\n",
    "        df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['song_id'] == 'SOOWVHQ12A8AE476A1']['song_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc829e",
   "metadata": {},
   "source": [
    "Read in the data:\n",
    "    \n",
    "    userID \\tab songID \\tab play_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1dec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_matrix = pd.read_csv('data/triplets_50000.txt', sep='\\t', names = ['userID','songID', 'play_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a27dca",
   "metadata": {},
   "source": [
    "Pivot to tranform the data from long to wide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c80853",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_matrix = u_matrix.pivot(index=\"userID\", columns=\"songID\", values=\"play_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb7e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_matrix[356:370]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e0362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_m = u_matrix.fillna(0) # Replace the NaN with 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count_list = u_matrix.sum(axis=1, skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c660c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The greatest listener's total play count is\", np.max(total_count_list))\n",
    "print(\"The lowest total play count of a user is\", np.min(total_count_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b475823",
   "metadata": {},
   "source": [
    "### Way num 1: normalize users and tranform to 0, 1\n",
    "\n",
    "So it's a good idea, before we start, to normalize for \"big\" and \"low\" listeners:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ad35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_normalized = u_matrix.sub(u_matrix.mean(axis=1, skipna=True), axis=0) # substract from each cell the row mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc44472",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f338685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b20b89",
   "metadata": {},
   "source": [
    "Lets use the rule:\n",
    "\n",
    "    if the norm play count is < 0 == the user didn't like the song\n",
    "    if the norm play count is > 0 == the user likes the song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e3f903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_binary = np.where(u_normalized[u_normalized.columns] < 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0840e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = u_normalized.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982fa696",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_[df_ >= 0] = 1\n",
    "df_[df_ < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb36f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c4aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_.T\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c44143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data for the LSH algo\n",
    "start_time = time.time()\n",
    "cols = df_.columns.to_numpy() # the users \n",
    "vectors_list = [cols[x].tolist() for x in df_.eq(1).to_numpy()] # each vector is a song, contains users that liked that song\n",
    "print('It took %s seconds.' %(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "len(vectors_list) # should be equal to the 27898 columns (songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b164c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame(vectors_list, index = df_.index)\n",
    "# df_new.shape # (27898, 49)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with none\n",
    "df_new_reduced = df_new.mask(df_new.eq('None')).dropna(how = 'all') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979aa7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_reduced['users'] = df_new_reduced[df_new_reduced.columns].apply(\n",
    "    lambda x: ','.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "df_new_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af192cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_new_reduced['users'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c1901",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e95c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a3204",
   "metadata": {},
   "source": [
    "Choose parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9dc190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Permutations\n",
    "permutations = 128\n",
    "\n",
    "#Number of Recommendations to return\n",
    "num_recommendations = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b60998",
   "metadata": {},
   "source": [
    "Create MiniHash forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d406f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess will split a string of text into individual tokens/shingles based on \",\".\n",
    "def preprocess(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf53085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forest(data, perms):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    minhash = []\n",
    "    \n",
    "    for users in data['users']:\n",
    "        tokens = preprocess(users) # list of users\n",
    "        m = MinHash(num_perm=perms)\n",
    "        for s in tokens:\n",
    "            m.update(s.encode('utf8'))\n",
    "        minhash.append(m)\n",
    "        \n",
    "    forest = MinHashLSHForest(num_perm=perms)\n",
    "    \n",
    "    for i,m in enumerate(minhash):\n",
    "        forest.add(i,m)\n",
    "        \n",
    "    forest.index()\n",
    "    \n",
    "    print('It took %s seconds to build forest.' %(time.time()-start_time))\n",
    "    \n",
    "    return forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = get_forest(data, permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ca466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(song_profile, database, perms, num_results, forest): # song_profile in list form\n",
    "    start_time = time.time()\n",
    "    \n",
    "    m = MinHash(num_perm=perms)\n",
    "    for users in song_profile:\n",
    "        m.update(users.encode('utf8'))\n",
    "        \n",
    "    idx_array = np.array(forest.query(m, num_results))\n",
    "    if len(idx_array) == 0:\n",
    "        return None # if your query is empty, return none\n",
    "    \n",
    "    result = database.iloc[idx_array]['users']\n",
    "    \n",
    "    print('It took %s seconds to query forest.' %(time.time()-start_time))\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d94d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_recommendations = 20\n",
    "song_profile = [ '5d5e0142e54c3bb7b69f548c2ee55066c90700eb'] # i made this random profile of an imaginary user manually, make it work with songID\n",
    "result = predict(song_profile, data, permutations, num_recommendations, forest)\n",
    "print('\\n Top Recommendation(s) is(are) \\n', result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654dbd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25448d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for song in result.index:\n",
    "    print(df.loc[df['song_id'] == song]['song_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b7f1d",
   "metadata": {},
   "source": [
    "## Way num. 2 pick for each song the top n users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34862341",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "t3 = u_matrix.T.apply(lambda x: pd.Series(x.nlargest(n).index), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387bb050",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u_matrix.T.shape,'\\n', t3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc734cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranform in the prefered format for the function \n",
    "c = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "t3['users'] = t3.iloc[:, :].apply(\",\".join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed9dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = t3.drop(t3.columns[range(0,10)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8094e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001cfbfd",
   "metadata": {},
   "source": [
    "Now we are ready to use the LSH function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08227501",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest2 = get_forest(data2, permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183af529",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_recommendations = 10\n",
    "user_profile = [ '00498f4bab2bfeb17680113c7d9525ad5b0ad401'] # i made this random profile of an imaginary user manually, make it work with songID\n",
    "result2 = predict(song_profile, data2, permutations, num_recommendations, forest2)\n",
    "print('\\n Top Recommendation(s) is(are) \\n', result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2efd8b",
   "metadata": {},
   "source": [
    "# Apriori algorithm based recommendation system\n",
    "\n",
    "\n",
    "https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/#apriori-frequent-itemsets-via-the-apriori-algorithm\n",
    "\n",
    "https://rasbt.github.io/mlxtend/\n",
    "\n",
    "Week 6 exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc5fb27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(data, columns=['artist_id', 'artist_name', 'artist_location', 'song_id', 'song_name', 'song_hottness','time_signature','artist_terms','artist_mbtags','mode','year','latitude','longitude'])\n",
    "with open('data/MillionSongSubset.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a6ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df.duplicated(['artist_id'], keep=False)]\n",
    "df1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9ba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "baskets = df1['artist_terms'].tolist()\n",
    "items = set(list(np.concatenate(baskets).flat))\n",
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c0091",
   "metadata": {},
   "outputs": [],
   "source": [
    "### hash all singletons\n",
    "df_item_hash = pd.DataFrame(range(len(items)), index = list(items), columns =['hashcode'], dtype=int)\n",
    "df_item_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd502d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### count the items, store the count into the hashed array index\n",
    "item_count_arr = np.zeros((len(baskets),1))\n",
    "\n",
    "for b in baskets:\n",
    "    for item in b:\n",
    "            idx = df_item_hash.loc[item,'hashcode']\n",
    "            item_count_arr[idx] += 1\n",
    "            \n",
    "### find frequent items with support > s1 (here s1 = 0.02), and hash back from array index to items           \n",
    "freq_items = [df_item_hash[df_item_hash['hashcode']==x].index[0] for x in np.where(item_count_arr > 0.02*len(baskets))[0]] \n",
    "print(len(freq_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f87e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq_item_hash = pd.DataFrame(range(1,len(freq_items)+1), index=freq_items, columns=['hashcode'])\n",
    "df_freq_item_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db22b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_mat_hashed = np.zeros((len(freq_items)+1,len(freq_items)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in baskets:\n",
    "    cand_list = [item for item in b if item in freq_items]\n",
    "    if len(cand_list)<2:\n",
    "        continue\n",
    "    for idx, item1 in enumerate(cand_list):\n",
    "        for item2 in cand_list[idx+1:]:\n",
    "            i = df_freq_item_hash.loc[item1,'hashcode'] \n",
    "            j = df_freq_item_hash.loc[item2,'hashcode'] \n",
    "            pair_mat_hashed[max(i,j),min(i,j)]+=1\n",
    "\n",
    "# pair_mat\n",
    "pair_mat_hashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c67de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_pairs = [[df_freq_item_hash[df_freq_item_hash['hashcode']==x].index[0], df_freq_item_hash[df_freq_item_hash['hashcode']==y].index[0]] for x, y in zip(*np.where(pair_mat_hashed > 0.02*len(baskets)))]\n",
    "freq_pairs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669ea4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(baskets).transform(baskets)\n",
    "df2 = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "apr = apriori(df2, min_support=0.05, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fe5f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "apr['length'] = apr['itemsets'].apply(lambda x: len(x))\n",
    "apr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca9d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_freq_pairs = []\n",
    "for i in apr[apr['length']==5].itemsets.values:\n",
    "    ml_freq_pairs.append(list(i))\n",
    "\n",
    "print(len(ml_freq_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733db686",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_terms_df = df['artist_terms']\n",
    "artist_terms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7bfd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET USERS TASTE\n",
    "#triples\n",
    "\n",
    "user_plays = pd.read_csv('data/triplets_50000.txt', sep='\\t', names = ['userID','songID', 'play_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dadfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a66c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_songs = user_plays['songID']\n",
    "myset_user = set(all_user_songs)\n",
    "print(len(myset_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac4e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_set = set(df1['song_id'])\n",
    "z = myset_user.intersection(songs_set)\n",
    "print(len(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f4ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_cleaned = df.loc[df['song_id'].isin(z)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d60df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/users_cleaned.zip'):\n",
    "    compression_opts = dict(method='zip', archive_name='out.csv')  \n",
    "    users_cleaned.to_csv('data/users_cleaned.zip', index=False, compression=compression_opts)  \n",
    "    songs_cleaned.to_csv('data/songs_cleaned.zip', index=False, compression=compression_opts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserSongTags(userID):\n",
    "    one_user_data = users_cleaned.loc[users_cleaned['userID'] == userID]\n",
    "    songs = one_user_data['songID']\n",
    "    tag_list = []\n",
    "    print(one_user_data)\n",
    "    \n",
    "    for song in songs:\n",
    "        song_data = songs_cleaned.loc[songs_cleaned['song_id'] == song]\n",
    "        tags = song_data['artist_terms']\n",
    "        tag_list.append(list(tags))\n",
    "    \n",
    "    return tag_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af34018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserSongTagsMULTIPLIED(userID):\n",
    "    one_user_data = users_cleaned.loc[users_cleaned['userID'] == userID]\n",
    "    songs = one_user_data['songID']\n",
    "    tag_list = []\n",
    "    \n",
    "    \n",
    "    for song in songs:\n",
    "        song_data = songs_cleaned.loc[songs_cleaned['song_id'] == song]\n",
    "        a = one_user_data.loc[one_user_data['songID'] == song]\n",
    "        song_play_count = a['play_count']\n",
    "        tags = song_data['artist_terms']\n",
    "        #print(song_play_count)\n",
    "        ints = int(song_play_count)\n",
    "        #print('AFTER : ', ints)\n",
    "        for i in range(0, (ints+1)):\n",
    "            tag_list.append(list(tags))\n",
    "            #print(i)\n",
    "        \n",
    "        #print(tags)\n",
    "        #print(song_play_count)\n",
    "    \n",
    "    return tag_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d8806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_tags = getUserSongTagsMULTIPLIED('8305c896f42308824da7d4386f4b9ee584281412')\n",
    "\n",
    "#print((user_tags))\n",
    "\n",
    "#print(user_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd885c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):\n",
    "    # initialize an empty string\n",
    "    str1 = \" \"\n",
    "   \n",
    "    # return string \n",
    "    sj = (str1.join(s))\n",
    "    return sj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTFIDFuser(userID):\n",
    "    user_tags = getUserSongTagsMULTIPLIED(userID)\n",
    "    #print(user_tags)\n",
    "    \n",
    "    tf_dc = {}\n",
    "    other_documents = []\n",
    "    porter = nltk.PorterStemmer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    doc = []\n",
    "    \n",
    "    for i in range(len(user_tags)):\n",
    "        var = user_tags[i][0]\n",
    "        #print(var)\n",
    "        doc.append(listToString(var))\n",
    "        #print(doc)\n",
    "\n",
    "    doc = listToString(doc)\n",
    "    other_documents.append(doc)\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # tokens = [porter.stem(w) for w in tokens]\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in tf_dc:\n",
    "            tf_dc[token] += 1\n",
    "        else:\n",
    "            tf_dc[token] = 1\n",
    "            \n",
    "    tf_dc = dict(sorted(tf_dc.items(), key=lambda item: item[1], reverse=True))\n",
    "    # We have only two documents\n",
    "    N = (len(user_tags)) \n",
    "    idf_dc = {}\n",
    "\n",
    "    for word in tf_dc.keys():\n",
    "        n = 0\n",
    "        for doc in other_documents:\n",
    "            if word in doc:\n",
    "                n += 1\n",
    "\n",
    "        idf_dc[word] = math.log(N / (n + 1)) + 1\n",
    "        \n",
    "    tf_idf_dc = {}\n",
    "    for word in tf_dc.keys():\n",
    "        tf_idf_dc[word] = tf_dc[word] * idf_dc[word]\n",
    "    \n",
    "    return tf_idf_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e5549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## This function is missing??????\n",
    "user_tf_if = getTFIDFuser('b80344d063b5ccb3212f76538f3d9e43d87dca9e')\n",
    "print(user_tf_if)\n",
    "\n",
    "for genre, freq in user_tf_if.items():\n",
    "    print(genre, ' : ', freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "first2pairs = {k: user_tf_if[k] for k in list(user_tf_if)[:5]}\n",
    "print(first2pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06762e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (26, 8), facecolor = None)\n",
    "plt.bar(user_tf_if.keys(), user_tf_if.values(), 1, color='g')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02565273",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tfidf = getTFIDFuser('b80344d063b5ccb3212f76538f3d9e43d87dca9e')\n",
    "\n",
    "\n",
    "first2pairs = {k: user_tfidf[k] for k in list(user_tfidf)[:7]}\n",
    "user_favourite_tags = []\n",
    "\n",
    "\n",
    "for genre, weight in first2pairs.items():\n",
    "    print(genre, ' : ',weight)\n",
    "    user_favourite_tags.append(genre)\n",
    "\n",
    "print(user_favourite_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb281f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "item_tags_for_finding_songs = []\n",
    "\n",
    "for items in ml_freq_pairs:\n",
    "    res = len(set(user_favourite_tags) & set(items))\n",
    "    uncommon_elements = set(user_favourite_tags) ^ set(items)\n",
    "    if(res >= 5 ):\n",
    "        #print(items)\n",
    "        item_tags_for_finding_songs.append(uncommon_elements)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab165d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "songid_matches = {}\n",
    "\n",
    "for index, row in songs_cleaned.iterrows():\n",
    "    \n",
    "    res = len(set(user_favourite_tags) & set(row['artist_terms']))\n",
    "    if(res >= 6 ):\n",
    "        songid_matches[row['song_id']] = res\n",
    "        #print(res)\n",
    "    \n",
    "    #uncommon_elements = set(user_favourite_tags) ^ set(items)\n",
    "    #print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = dict(sorted(songid_matches.items(), key=lambda x: x[1],  reverse=True) )\n",
    "top10recommended = {k: a[k] for k in list(a)[:10]}\n",
    "\n",
    "top10recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5837075",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list1 = [5, 6, 4, 10, 7, 1, 19]\n",
    "test_list2 = [6, 6, 10, 3, 7, 10, 19]\n",
    " \n",
    "# printing original lists\n",
    "print(\"The original list 1 is : \" + str(test_list1))\n",
    "print(\"The original list 2 is : \" + str(test_list2))\n",
    " \n",
    "# Identical element summation in lists\n",
    "# using set() + len()\n",
    "res = len(set(test_list1) & set(test_list2));\n",
    " \n",
    "# printing result\n",
    "print(\"Summation of Identical elements : \" + str(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f458ff23",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "* song is a good recommendation if it is the same genre\n",
    "\n",
    "we decide that a it is a good recommendation if half the genres overlap. Between the query and the recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a9552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(query, recommendations):\n",
    "    K = 10 # number of retrieved items to query song\n",
    "    aps = []\n",
    "    for i, song in enumerate(recommended_songs):\n",
    "        p = np.zeros(K)  # precisions at k\n",
    "        r = np.zeros(K)  # recalls at k\n",
    "        df.song[]\n",
    "        y_true = query # genres of the query\n",
    "        y_pred = 1 # genres of the recommendation\n",
    "\n",
    "        # k ranking\n",
    "        for k in range(1, K+1):\n",
    "            tp = np.sum((y_true == y_pred[:k]))\n",
    "\n",
    "            p[k-1] = tp/len(y_pred[:k])\n",
    "            # fraction of objects predicted to be positive among all positive objects\n",
    "            r[k-1] = tp/K\n",
    "            # True Positive Identification Rate (TPIR): \n",
    "            # Probability of observing the correct identity within the top K ranks\n",
    "\n",
    "        # binarize predictions\n",
    "        y_pred[y_pred != y_true] = 0\n",
    "        y_pred[y_pred == y_true] = 1\n",
    "\n",
    "        ap = 1/(y_pred.sum() + 1e-9) * (p @ y_pred)\n",
    "        aps.append(ap)\n",
    "\n",
    "    maP = np.mean(aps)\n",
    "    return maP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08614e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_to_\n",
    "for user in users:\n",
    "    recommendation = get_recommendation(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9a226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4f92193806e2908606a5f23edd55a5282f2f433b73b1c504507f9256ed9f0b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
